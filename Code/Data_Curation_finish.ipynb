{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# file 을 나누기"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_Pickle(set_data, set_filename, force = False):\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "        # You may override by setting force=True.  \n",
    "        print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else :\n",
    "        try :\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(set_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e :\n",
    "            print(\"Unable to save data to\", set_filename, ': ', e)\n",
    "            return\n",
    "\n",
    "def make_folders():\n",
    "    set_dir_name = \"pickles/multiprocessing_origin/\"\n",
    "\n",
    "    if os.path.exists(set_dir_name):\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_dir_name)\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(set_dir_name)\n",
    "        except Exception as e:\n",
    "            print('Unable to make', set_dir_name, ':', e)\n",
    "            return\n",
    "\n",
    "def open_Pickle(pickle_path):\n",
    "    try :\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            train = pickle.load(f)\n",
    "            return train\n",
    "    except Exception as e :\n",
    "        print(\"Unable to save data to\", pickle_path, ': ', e)\n",
    "        return\n",
    "            \n",
    "def seperate_indi(groups):\n",
    "    nFiles = round(len(groups)/cores)\n",
    "    \n",
    "    print(nFiles)\n",
    "    \n",
    "    # set_dir_name = \"pickles/individuals/process{0}\".format(i)\n",
    "    \n",
    "    groups_dict = dict(list(groups))\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(cores):\n",
    "        set_file_name = \"pickles/multiprocessing_origin/process{0}.pickle\".format(i)\n",
    "        \n",
    "        if count < cores-1:\n",
    "            gb = pd.concat(list(groups_dict.values())[nFiles*i:nFiles*(i+1)])\n",
    "            make_Pickle(gb, set_file_name)\n",
    "            print('inside')\n",
    "        else:\n",
    "            print('last!')\n",
    "            gb = pd.concat(list(groups_dict.values())[nFiles*i:])\n",
    "            make_Pickle(gb, set_file_name)\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - importing data \n",
      "\n",
      " -Data has 4005 individualas with 13835 data\n",
      "\n",
      " - Seperate data into 4 groups including individuals\n",
      "1001\n",
      "inside\n",
      "inside\n",
      "inside\n",
      "last!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #data analysis\n",
    "# import os #file/directory operations\n",
    "import multiprocessing as mp\n",
    "import Datacuration as dc\n",
    "# from six.moves import cPickle as pickle\n",
    "\n",
    "datafile='../../data/v2merged_test.csv' #full dataset\n",
    "df=pd.read_csv(datafile)\n",
    "print(\"\\n - importing data \")\n",
    "        \n",
    "groups = df.groupby('ga:eventAction')\n",
    "print(\"\\n - Data has {0} individualas with {1} data\".format(len(groups), len(df)))\n",
    "\n",
    "cores = mp.cpu_count() # number of cores\n",
    "\n",
    "dc.make_folders(\"../../data/pickles/multiprocessing_test/\") # grouping data based on core number\n",
    "print(\"\\n - Seperate data into {0} groups including individuals\".format(cores, round(len(groups)/cores)))\n",
    "\n",
    "dc.seperate_indi(groups, cores, \"../../data/pickles/multiprocessing_test/process{0}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sleepgap.py를 실행해 result_sleepgap_15.csv 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug  2 12:03:48 2016\n",
    "\n",
    "@author: coq15sg\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np #numerical function\n",
    "import pandas as pd #data analysis\n",
    "import datetime \n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Queue\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "t0 = time.clock() # initial time\n",
    "cores= mp.cpu_count() #processes raw data in batches (required because data>ram)\n",
    "loding = True\n",
    "\n",
    "# parameters\n",
    "long_gap_length=7*60 \n",
    "gap_window=5*60 #ie long gaps are 7 hours<gap<11 hours\n",
    "shor_gap_length=15 #analysis extremely robust to changes in this parameter\n",
    "#guesses at what bounds waking time across the population\n",
    "early_rising=5\n",
    "late_rising=12\n",
    "#guesses at what bounds bedtime across the population\n",
    "early_daytim=early_rising+12\n",
    "late_daytim=late_rising+12\n",
    "\n",
    "def open_Pickle(pickle_path):\n",
    "    try :\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            train = pickle.load(f)\n",
    "            return train\n",
    "    except Exception as e :\n",
    "        print(\"Unable to save data to\", pickle_path, ': ', e)\n",
    "        return\n",
    "\n",
    "def add_TotalPlays(data_frame):\n",
    "    # add players total number of attempts to df\n",
    "    data_frame['total_plays'] = len(data_frame)\n",
    "    #max(data_frame['ga:eventLabel'])\n",
    "    return data_frame\n",
    "\n",
    "def add_GapType(df):\n",
    "    df['localtime']=np.around((df['ga:hour']+df['ga:longitude']*24/360)%24 , decimals=1)\n",
    "    df['long_gap']=(df['diff_time']>long_gap_length) & (df['diff_time']<(long_gap_length+gap_window))\n",
    "    df['shor_gap']=(df['diff_time']<shor_gap_length)\n",
    "    df['sleepgap']=df['long_gap'] & ((df['localtime']>early_rising) & (df['localtime']<late_rising))\n",
    "    return df\n",
    "\n",
    "def add_GapCategory(df):\n",
    "    '''categorise people by their gapness'''\n",
    "    '''here define range over which we define taking gaps'''\n",
    "    #dfp=df[6:8] #tight range, on game 7 \n",
    "    dfp=df[1:15] #any of first 15 games   \n",
    "    \n",
    "    if len(dfp.shor_gap)==sum(dfp.shor_gap):\n",
    "        gaptype=1\n",
    "    elif (sum(dfp.long_gap)==1) & (sum(dfp.sleepgap)==0):\n",
    "        gaptype=2\n",
    "    elif (sum(dfp.long_gap)==1) & (sum(dfp.sleepgap)==1):\n",
    "        gaptype=3\n",
    "    else:\n",
    "        gaptype=4\n",
    "        \n",
    "    df['gaptype']=gaptype    \n",
    "        \n",
    "    return df\n",
    "\n",
    "def add_DiffTime(df):\n",
    "    for index, row in df.iterrows():\n",
    "        '''\n",
    "        if not index%10:\n",
    "            print(\"{0}: {1}%\".format(str(queue), round(index/len(new_df)*100)))\n",
    "            \n",
    "        #count = count+1\n",
    "        '''\n",
    "        ## combine day, hour and minute into datetime object (ie absolute time)\n",
    "        str_date_time = \"{0}:{1}:{2}\".format(str(int(row['ga:date'])), str(int(row['ga:hour'])), str(int(row['ga:minute'])))\n",
    "        str_to_time = datetime.datetime.strptime(str_date_time, \"%Y%m%d:%H:%M\")\n",
    "        # print(\"{0}: {1}\".format(index, aa))\n",
    "\n",
    "        df.loc[index, 'comb_time'] = str_to_time\n",
    "        \n",
    "        if df.loc[index,'ga:eventLabel']==1:\n",
    "            df.loc[index, 'diff_time'] = np.NaN\n",
    "        else:\n",
    "            try:\n",
    "                diff_time = (df.loc[index,'comb_time'].to_datetime() - df.loc[index-1,'comb_time'].to_datetime()).total_seconds()/60\n",
    "                df.loc[index, 'diff_time'] = diff_time\n",
    "            except: #Exception as e:\n",
    "                #print(\"Index is -1: \", e)\n",
    "                df.loc[index, 'diff_time'] = np.NaN\n",
    "    return df\n",
    "\n",
    "def check_Discon(df, name_vec):    \n",
    "    count = 0\n",
    "    groups = df.groupby('ga:eventAction')\n",
    "\n",
    "    for name, group in groups:\n",
    "        df_len = len(group)\n",
    "        eventLabel = np.ones(df_len)\n",
    "        eventLabel[:] = group['ga:eventLabel']\n",
    "        diff_AlOm = eventLabel[df_len-1]-eventLabel[0]+1\n",
    "\n",
    "        if not diff_AlOm == df_len:\n",
    "            #print(\"{0},\".format(name))\n",
    "            #print(\"{0}, length:{1}, difference:{2}\".format(name, df_len, diff_AlOm))\n",
    "            #print(type(name_vec))\n",
    "            name_vec.append(name)\n",
    "        count = count+1\n",
    "\n",
    "def filter_time(df):\n",
    "    #pre=len(df)\n",
    "    df=df[~(df['ga:latitude']==0)]\n",
    "    #print(\"latitude!=0 filter reduces players from \" + str(pre) + \" to \" + str(len(df)) + \" = \" + format(round(len(df)/float(pre),2)))\n",
    "\n",
    "    '''remove data with negative time differences'''\n",
    "    '''cannot work out why some time differences are <0, but have verified that >0 values are correct'''\n",
    "    #pre=len(df)\n",
    "    df=df[~(df.diff_time<0)]\n",
    "    #print(\"difftime>0 filter reduces players from \" + str(pre) + \" to \" + str(len(df)) + \" = \" + format(round(len(df)/float(pre),2)))\n",
    "\n",
    "    return df\n",
    "\n",
    "def rm_GaInCols(data_frame): # remove ga:\n",
    "    cols = np.array(data_frame.columns)\n",
    "    count = 0\n",
    "    for elmt in cols:\n",
    "        if 'ga:' in elmt:\n",
    "            cols[count] = elmt.strip('ga:')\n",
    "        count = count + 1\n",
    "    return cols\n",
    "\n",
    "def process_df(queue, file_name):\n",
    "    err_players = []\n",
    "    \n",
    "    df = open_Pickle(file_name)\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    new_df = df.groupby('ga:eventAction').apply(add_TotalPlays)\n",
    "    new_df = new_df[new_df['total_plays']>14]\n",
    "    new_df = new_df[new_df['ga:eventLabel']<301]\n",
    "    check_Discon(new_df, err_players)\n",
    "                  \n",
    "    new_df = add_DiffTime(new_df)\n",
    "    new_df = add_GapType(new_df)\n",
    "    #df = filter_time(df) # 이건 안하는게 좋겠다. attemps를 지우니까.\n",
    "    new_df=new_df.groupby('ga:eventAction').apply(add_GapCategory)\n",
    "                  \n",
    "    queue.put([new_df, err_players])\n",
    "\n",
    "## Execute! ##\n",
    "#print(\"\\n - Data Accommodating\")\n",
    "print('\\n -  Data Loading...')\n",
    "# start multiprocessing\n",
    "for i in range(cores):\n",
    "    set_filename = \"pickles/multiprocessing_test/process{0}.pickle\".format(i)\n",
    "    set_pros_name = \"core{0}\".format(i)\n",
    "    set_q_name = \"q{0}\".format(i)\n",
    "\n",
    "    globals()[set_q_name] = Queue()\n",
    "    globals()[set_pros_name] = Process(target=process_df, args=(eval(set_q_name), set_filename,))\n",
    "    #globals()[set_pros_name] = Process(target=process_df, args=(eval(set_q_name), set_filename, eval(error_name),))\n",
    "\n",
    "    eval(set_pros_name).start()\n",
    "    print('core{0} start'.format(i))\n",
    "    time.sleep(1)\n",
    "\n",
    "print('\\n -  Data Loading Finish...')\n",
    "## get returned value from multiprocessing\n",
    "df = pd.DataFrame()\n",
    "err_plys = pd.DataFrame()\n",
    "for i in range(cores):\n",
    "    set_q_name = \"q{0}\".format(i) # queue name\n",
    "    set_pros_name = \"core{}\".format(i) # process name\n",
    "    set_df_name = \"df{0}\".format(i) # data frame name of each process\n",
    "    error_name = \"error_players{0}\".format(i) # player name with error attempts\n",
    "    \n",
    "    locals()[set_df_name], locals()[error_name] = eval(set_q_name).get() # get from queue\n",
    "\n",
    "    df = df.append(eval(set_df_name)) # concat each dfs\n",
    "    err_plys = err_plys.append(eval(error_name)) # np.hstack((err_plys, eval(error_name))) # concat each error players\n",
    "    #df = df.drop('level_0', axis=1)\n",
    "    \n",
    "    eval(set_q_name).close()\n",
    "    eval(set_pros_name).join()\n",
    "    print('core{0} close'.format(i))\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n - sorting...\")\n",
    "df.sort_values(by=['ga:eventAction', 'ga:eventLabel'], inplace=True) # mac\n",
    "#df.sort(columns=['ga:eventAction', 'ga:eventLabel'],inplace=True) # school computer\n",
    "df = df.reset_index()\n",
    "err_plys = err_plys.reset_index()\n",
    "\n",
    "df = df.drop('index', axis=1)\n",
    "df.columns = rm_GaInCols(df) # remove ga:\n",
    "\n",
    "err_plys = err_plys.drop('index', axis=1)\n",
    "err_plys.columns = ['player ID']\n",
    "\n",
    "print(\"\\n - saving processed data...\\n {0} plays, {1}players\".format(len(df), len(df.groupby('eventAction'))))\n",
    "\n",
    "df.to_csv('filtered_data_test.csv')\n",
    "err_plys.to_csv('error_players_test.csv')\n",
    "print(\"\\n - process terminal.\")\n",
    "print(\"Run time: \", time.clock()-t0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(err_plys)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash table style\n",
    "\n",
    "import pandas as pd #data analysis\n",
    "import numpy as np #numerical function\n",
    "from six.moves import cPickle as pickle\n",
    "df=pd.read_csv('filtered_data_test.csv')\n",
    "print(df.columns)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.drop('index', axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a matrix that will contain each column of each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np #numerical function\n",
    "import pandas as pd #data analysis\n",
    "import os #file/directory operations\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "df=pd.read_csv('filtered_data_test.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spt_ColData(data_frame, col_name): # seperate columns into each matrix\n",
    "    '''\n",
    "    def group_Len(data, len_vec): # inner function\n",
    "        len_vec.append(len(data))\n",
    "\n",
    "    group_len = []\n",
    "    groups = df.groupby('ga:eventAction') # grouping\n",
    "    groups.apply(group_Len, group_len) # get length of each player\n",
    "\n",
    "    #print(max(group_len), len(groups)) # row = attemt numbers, column = players\n",
    "    '''\n",
    "    groups = df.groupby('eventAction') # grouping\n",
    "    col_matrix = np.ones((301, len(groups)))*np.nan # make a matrix with player attempts size.\n",
    "    \n",
    "    #if type(group[col_name]) == str:\n",
    "    #col_matrix = np.chararray((301, len(groups)), itemsize=37)\n",
    "    #col_matrix = np.zeros((301, len(groups)))\n",
    "\n",
    "    count = 0\n",
    "    for name, group in groups:\n",
    "        idx = np.array(group['eventLabel'])-1\n",
    "        col_matrix[idx, count] = group[col_name]\n",
    "        count = count + 1\n",
    "    #print('Matrix size:', np.shape(col_matrix))\n",
    "    return col_matrix\n",
    "\n",
    "def make_folders(set_dir_name):\n",
    "    if os.path.exists(set_dir_name):\n",
    "      # You may override by setting force=True.\n",
    "      print('%s folder already present - Skipping pickling.' % set_dir_name)\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(set_dir_name)\n",
    "        except Exception as e:\n",
    "            print('Unable to make', set_dir_name, ':', e)\n",
    "            return\n",
    "\n",
    "def make_Pickle(set_data, set_filename, force = False):\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "        # You may override by setting force=True.  \n",
    "        print('%s pickle already present - Skipping pickling.' % set_filename)\n",
    "    else :\n",
    "        try :\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(set_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e :\n",
    "            print(\"Unable to save data to\", set_filename, ': ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(df.columns)\n",
    "mm = spt_ColData(df, 'eventLabel')\n",
    "#mm = mm[:15, :]\n",
    "print(mm[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conver to picke except 'eventAction' and 'comb_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def colToPickle():\n",
    "    set_folder_name = 'pickles/seperate_origin/'\n",
    "    make_folders(set_folder_name)    \n",
    "    \n",
    "    for col_name in np.array(df.columns):\n",
    "        if not (col_name == 'eventAction' or col_name == 'comb_time'): # conver to picke except 'eventAction' and 'comb_time'\n",
    "            set_mat = col_name\n",
    "            print(col_name)\n",
    "            locals()[set_mat] = spt_ColData(df, col_name)\n",
    "            make_Pickle(eval(set_mat), 'pickles/seperate_origin/{0}'.format(col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colToPickle()\n",
    "\n",
    "groups = df.groupby('eventAction')\n",
    "names = pd.DataFrame({'ID':groups.grouper.result_index.values})\n",
    "make_Pickle(names, 'pickles/seperate_origin/eventAction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
