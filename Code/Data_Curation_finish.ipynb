{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Seperate File for Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd #data analysis\n",
    "# import os #file/directory operations\n",
    "import multiprocessing as mp\n",
    "import DC_Pickle as dcp\n",
    "# from six.moves import cPickle as pickle\n",
    "\n",
    "datafile='../../data/v2merged_test.csv' #full dataset\n",
    "df=pd.read_csv(datafile)\n",
    "print(\"\\n - importing data... \")\n",
    "        \n",
    "groups = df.groupby('ga:eventAction')\n",
    "print(\"\\n - Data has {0} individualas with {1} data\".format(len(groups), len(df)))\n",
    "\n",
    "cores = mp.cpu_count() # number of cores\n",
    "\n",
    "dcp.make_folders(\"../../data/pickles/multiprocessing_test/\") # grouping data based on core number\n",
    "print(\"\\n - Seperate data into {0} groups including individuals\".format(cores, round(len(groups)/cores)))\n",
    "\n",
    "dcp.seperate_indi(groups, cores, \"../../data/pickles/multiprocessing_test/process{0}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sleepgap.py를 실행해 result_sleepgap_15.csv 파일 생성"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# parameters\n",
    "long_gap_length=7*60 \n",
    "gap_window=5*60 #ie long gaps are 7 hours<gap<11 hours\n",
    "shor_gap_length=15 #analysis extremely robust to changes in this parameter\n",
    "#guesses at what bounds waking time across the population\n",
    "early_rising=5\n",
    "late_rising=12\n",
    "#guesses at what bounds bedtime across the population\n",
    "early_daytim=early_rising+12\n",
    "late_daytim=late_rising+12\n",
    "\n",
    "\n",
    "def add_TotalPlays(data_frame):\n",
    "    # add players total number of attempts to df\n",
    "    data_frame['total_plays'] = len(data_frame)\n",
    "    #max(data_frame['ga:eventLabel'])\n",
    "    return data_frame\n",
    "\n",
    "def add_GapType(df):\n",
    "    df['localtime']=np.around((df['ga:hour']+df['ga:longitude']*24/360)%24 , decimals=1)\n",
    "    df['long_gap']=(df['diff_time']>long_gap_length) & (df['diff_time']<(long_gap_length+gap_window))\n",
    "    df['shor_gap']=(df['diff_time']<shor_gap_length)\n",
    "    df['sleepgap']=df['long_gap'] & ((df['localtime']>early_rising) & (df['localtime']<late_rising))\n",
    "    return df\n",
    "\n",
    "def add_GapCategory(df):\n",
    "    '''categorise people by their gapness'''\n",
    "    '''here define range over which we define taking gaps'''\n",
    "    #dfp=df[6:8] #tight range, on game 7 \n",
    "    dfp=df[1:15] #any of first 15 games   \n",
    "    \n",
    "    if len(dfp.shor_gap)==sum(dfp.shor_gap):\n",
    "        gaptype=1\n",
    "    elif (sum(dfp.long_gap)==1) & (sum(dfp.sleepgap)==0):\n",
    "        gaptype=2\n",
    "    elif (sum(dfp.long_gap)==1) & (sum(dfp.sleepgap)==1):\n",
    "        gaptype=3\n",
    "    else:\n",
    "        gaptype=4\n",
    "        \n",
    "    df['gaptype']=gaptype    \n",
    "        \n",
    "    return df\n",
    "\n",
    "def add_DiffTime(df):\n",
    "    for index, row in df.iterrows():\n",
    "        '''\n",
    "        if not index%10:\n",
    "            print(\"{0}: {1}%\".format(str(queue), round(index/len(new_df)*100)))\n",
    "            \n",
    "        #count = count+1\n",
    "        '''\n",
    "        ## combine day, hour and minute into datetime object (ie absolute time)\n",
    "        str_date_time = \"{0}:{1}:{2}\".format(str(int(row['ga:date'])), str(int(row['ga:hour'])), str(int(row['ga:minute'])))\n",
    "        str_to_time = datetime.datetime.strptime(str_date_time, \"%Y%m%d:%H:%M\")\n",
    "        # print(\"{0}: {1}\".format(index, aa))\n",
    "\n",
    "        df.loc[index, 'comb_time'] = str_to_time\n",
    "        \n",
    "        if df.loc[index,'ga:eventLabel']==1:\n",
    "            df.loc[index, 'diff_time'] = np.NaN\n",
    "        else:\n",
    "            try:\n",
    "                diff_time = (df.loc[index,'comb_time'].to_datetime() - df.loc[index-1,'comb_time'].to_datetime()).total_seconds()/60\n",
    "                df.loc[index, 'diff_time'] = diff_time\n",
    "            except: #Exception as e:\n",
    "                #print(\"Index is -1: \", e)\n",
    "                df.loc[index, 'diff_time'] = np.NaN\n",
    "    return df\n",
    "\n",
    "def check_Discon(df, name_vec):    \n",
    "    count = 0\n",
    "    groups = df.groupby('ga:eventAction')\n",
    "\n",
    "    for name, group in groups:\n",
    "        df_len = len(group)\n",
    "        eventLabel = np.ones(df_len)\n",
    "        eventLabel[:] = group['ga:eventLabel']\n",
    "        diff_AlOm = eventLabel[df_len-1]-eventLabel[0]+1\n",
    "\n",
    "        if not diff_AlOm == df_len:\n",
    "            #print(\"{0},\".format(name))\n",
    "            #print(\"{0}, length:{1}, difference:{2}\".format(name, df_len, diff_AlOm))\n",
    "            #print(type(name_vec))\n",
    "            name_vec.append(name)\n",
    "        count = count+1\n",
    "\n",
    "def filter_time(df):\n",
    "    #pre=len(df)\n",
    "    df=df[~(df['ga:latitude']==0)]\n",
    "    #print(\"latitude!=0 filter reduces players from \" + str(pre) + \" to \" + str(len(df)) + \" = \" + format(round(len(df)/float(pre),2)))\n",
    "\n",
    "    '''remove data with negative time differences'''\n",
    "    '''cannot work out why some time differences are <0, but have verified that >0 values are correct'''\n",
    "    #pre=len(df)\n",
    "    df=df[~(df.diff_time<0)]\n",
    "    #print(\"difftime>0 filter reduces players from \" + str(pre) + \" to \" + str(len(df)) + \" = \" + format(round(len(df)/float(pre),2)))\n",
    "\n",
    "    return df\n",
    "\n",
    "def rm_GaInCols(data_frame): # remove ga:\n",
    "    cols = np.array(data_frame.columns)\n",
    "    count = 0\n",
    "    for elmt in cols:\n",
    "        if 'ga:' in elmt:\n",
    "            cols[count] = elmt.strip('ga:')\n",
    "        count = count + 1\n",
    "    return cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       shor_gap\n",
      "0            15\n",
      "1            15\n",
      "2            15\n",
      "3            15\n",
      "4            15\n",
      "5            15\n",
      "6            15\n",
      "7            15\n",
      "8            15\n",
      "9            15\n",
      "10           15\n",
      "11           15\n",
      "12           15\n",
      "13           15\n",
      "14           15\n",
      "15           15\n",
      "16           15\n",
      "17           15\n",
      "18           15\n",
      "19           15\n",
      "20           15\n",
      "21           15\n",
      "22           15\n",
      "23           15\n",
      "24           15\n",
      "25           15\n",
      "26           15\n",
      "27           15\n",
      "28           15\n",
      "29           15\n",
      "...         ...\n",
      "13805        15\n",
      "13806        15\n",
      "13807        15\n",
      "13808        15\n",
      "13809        15\n",
      "13810        15\n",
      "13811        15\n",
      "13812        15\n",
      "13813        15\n",
      "13814        15\n",
      "13815        15\n",
      "13816        15\n",
      "13817        15\n",
      "13818        15\n",
      "13819        15\n",
      "13820        15\n",
      "13821        15\n",
      "13822        15\n",
      "13823        15\n",
      "13824        15\n",
      "13825        15\n",
      "13826        15\n",
      "13827        15\n",
      "13828        15\n",
      "13829        15\n",
      "13830        15\n",
      "13831        15\n",
      "13832        15\n",
      "13833        15\n",
      "13834        15\n",
      "\n",
      "[13835 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "datafile='../../data/v2merged_test.csv' #full dataset\n",
    "df=pd.read_csv(datafile)\n",
    "shor_gap_length=15 #analysis extremely robust to changes in this parameter\n",
    "\n",
    "#shor_gap_length1 = pd.DataFrame(np.ones, columns='aa')\n",
    "\n",
    "shor_gap_length1 = pd.DataFrame({'shor_gap':np.ones(len(df))*shor_gap_length})\n",
    "print(shor_gap_length1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd #data analysis\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Queue\n",
    "import DC_Pickle as dcp\n",
    "import DC_filter as dcf\n",
    "t0 = time.clock() # initial time\n",
    "cores= mp.cpu_count() #processes raw data in batches (required because data>ram)\n",
    "loding = True\n",
    "\n",
    "def process_df(queue, file_name):\n",
    "    err_players = []\n",
    "    \n",
    "    df = dcp.open_Pickle(file_name)\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "    new_df = df.groupby('ga:eventAction').apply(dcf.add_TotalPlays)\n",
    "    new_df = new_df[new_df['total_plays']>14]\n",
    "    new_df = new_df[new_df['ga:eventLabel']<301]\n",
    "    dcf.check_Discon(new_df, err_players)\n",
    "                  \n",
    "    new_df = dcf.add_DiffTime(new_df)\n",
    "    new_df = dcf.add_GapType(new_df)\n",
    "    #df = filter_time(df) # 이건 안하는게 좋겠다. attemps를 지우니까.\n",
    "    new_df=new_df.groupby('ga:eventAction').apply(dcf.add_GapCategory)\n",
    "                  \n",
    "    queue.put([new_df, err_players])\n",
    "\n",
    "## Execute! ##\n",
    "print('\\n -  Data Loading...')\n",
    "\n",
    "## start multiprocessing\n",
    "for i in range(cores):\n",
    "    set_filename = \"../../data/pickles/multiprocessing_test/process{0}.pickle\".format(i)\n",
    "    set_pros_name = \"core{0}\".format(i)\n",
    "    set_q_name = \"q{0}\".format(i)\n",
    "\n",
    "    globals()[set_q_name] = Queue()\n",
    "    globals()[set_pros_name] = Process(target=process_df, args=(eval(set_q_name), set_filename,))\n",
    "    #globals()[set_pros_name] = Process(target=process_df, args=(eval(set_q_name), set_filename, eval(error_name),))\n",
    "\n",
    "    eval(set_pros_name).start()\n",
    "    print('core{0} start'.format(i))\n",
    "    time.sleep(1)\n",
    "\n",
    "print('\\n -  Data filtering...')\n",
    "\n",
    "## get returned value from multiprocessing\n",
    "df = pd.DataFrame()\n",
    "err = pd.DataFrame()\n",
    "for i in range(cores):\n",
    "    set_q_name = \"q{0}\".format(i) # queue name\n",
    "    set_pros_name = \"core{}\".format(i) # process name\n",
    "    set_df_name = \"df{0}\".format(i) # data frame name of each process\n",
    "    error_name = \"error_players{0}\".format(i) # player name with error attempts\n",
    "    \n",
    "    locals()[set_df_name], locals()[error_name] = eval(set_q_name).get() # get from queue\n",
    "\n",
    "    df = df.append(eval(set_df_name)) # concat each dfs\n",
    "    err = err.append(eval(error_name)) # np.hstack((err_plys, eval(error_name))) # concat each error players\n",
    "    #df = df.drop('level_0', axis=1)\n",
    "    \n",
    "    eval(set_q_name).close()\n",
    "    eval(set_pros_name).join()\n",
    "    print('core{0} close'.format(i))\n",
    "    time.sleep(1)\n",
    "\n",
    "[df, err] = dcf.data_PostProcess(df, err)\n",
    "\n",
    "print(\"\\n - saving processed data...\\n {0} plays, {1}players\".format(len(df), len(df.groupby('eventAction'))))\n",
    "\n",
    "df.to_csv('../../data/filtered_data_test.csv')\n",
    "err.to_csv('../../data/error_players_test.csv')\n",
    "print(\"\\n - process terminal.\")\n",
    "print(\"Run time: \", time.clock()-t0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(err_plys)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash table style\n",
    "\n",
    "import pandas as pd #data analysis\n",
    "import numpy as np #numerical function\n",
    "from six.moves import cPickle as pickle\n",
    "df=pd.read_csv('filtered_data_test.csv')\n",
    "print(df.columns)\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df = df.drop('index', axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a matrix that will contain each column of each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np #numerical function\n",
    "import pandas as pd #data analysis\n",
    "import os #file/directory operations\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "df=pd.read_csv('filtered_data_test.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spt_ColData(data_frame, col_name): # seperate columns into each matrix\n",
    "    '''\n",
    "    def group_Len(data, len_vec): # inner function\n",
    "        len_vec.append(len(data))\n",
    "\n",
    "    group_len = []\n",
    "    groups = df.groupby('ga:eventAction') # grouping\n",
    "    groups.apply(group_Len, group_len) # get length of each player\n",
    "\n",
    "    #print(max(group_len), len(groups)) # row = attemt numbers, column = players\n",
    "    '''\n",
    "    groups = df.groupby('eventAction') # grouping\n",
    "    col_matrix = np.ones((301, len(groups)))*np.nan # make a matrix with player attempts size.\n",
    "    \n",
    "    #if type(group[col_name]) == str:\n",
    "    #col_matrix = np.chararray((301, len(groups)), itemsize=37)\n",
    "    #col_matrix = np.zeros((301, len(groups)))\n",
    "\n",
    "    count = 0\n",
    "    for name, group in groups:\n",
    "        idx = np.array(group['eventLabel'])-1\n",
    "        col_matrix[idx, count] = group[col_name]\n",
    "        count = count + 1\n",
    "    #print('Matrix size:', np.shape(col_matrix))\n",
    "    return col_matrix\n",
    "\n",
    "def make_folders(set_dir_name):\n",
    "    if os.path.exists(set_dir_name):\n",
    "      # You may override by setting force=True.\n",
    "      print('%s folder already present - Skipping pickling.' % set_dir_name)\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(set_dir_name)\n",
    "        except Exception as e:\n",
    "            print('Unable to make', set_dir_name, ':', e)\n",
    "            return\n",
    "\n",
    "def make_Pickle(set_data, set_filename, force = False):\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "        # You may override by setting force=True.  \n",
    "        print('%s pickle already present - Skipping pickling.' % set_filename)\n",
    "    else :\n",
    "        try :\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(set_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e :\n",
    "            print(\"Unable to save data to\", set_filename, ': ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(df.columns)\n",
    "mm = spt_ColData(df, 'eventLabel')\n",
    "#mm = mm[:15, :]\n",
    "print(mm[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conver to picke except 'eventAction' and 'comb_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def colToPickle():\n",
    "    set_folder_name = 'pickles/seperate_origin/'\n",
    "    make_folders(set_folder_name)    \n",
    "    \n",
    "    for col_name in np.array(df.columns):\n",
    "        if not (col_name == 'eventAction' or col_name == 'comb_time'): # conver to picke except 'eventAction' and 'comb_time'\n",
    "            set_mat = col_name\n",
    "            print(col_name)\n",
    "            locals()[set_mat] = spt_ColData(df, col_name)\n",
    "            make_Pickle(eval(set_mat), 'pickles/seperate_origin/{0}'.format(col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colToPickle()\n",
    "\n",
    "groups = df.groupby('eventAction')\n",
    "names = pd.DataFrame({'ID':groups.grouper.result_index.values})\n",
    "make_Pickle(names, 'pickles/seperate_origin/eventAction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
