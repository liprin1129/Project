{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using competitive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Dimension: 15\n",
      " - Individuals: 69\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math\n",
    "import DC_Pickle as dcp\n",
    "\n",
    "train_scores = dcp.open_Pickle('../../data/pickles/seperate_test/eventValue.pickle')\n",
    "train_scores = train_scores[:15, :]\n",
    "\n",
    "train_data = np.ones(15)*np.nan\n",
    "idx_data = []\n",
    "\n",
    "for i in range(train_scores.shape[1]):\n",
    "    if not np.isnan(train_scores[:, i]).any():\n",
    "        train_data = np.vstack((train_data, train_scores[:, i]))\n",
    "        idx_data.append(i)\n",
    "        \n",
    "train_data = numpy.delete(train_data, (0), axis=0) # delete the first row including nan\n",
    "train_data = train_data.T\n",
    "\n",
    "[n,m]  = np.shape(train_data)               # dimension and number of training data\n",
    "\n",
    "eta    = 0.05                               # learning rate\n",
    "winit  = 1                                  # initialise weight\n",
    "\n",
    "tmax   = 40000                              # max learning iteration\n",
    "clusters = 10                               # number of cluster\n",
    "\n",
    "print(\" - Dimension: {0}\\n - Individuals: {1}\".format(n, m))\n",
    "\n",
    "## function for clustering with cost function return\n",
    "def get_Cost(clusters_num, train):\n",
    "    costs = []\n",
    "    centroids = np.zeros((clusters_num, n, 10), dtype=float)\n",
    "    cluster_idx = np.ones((10, m))*np.nan # 2X2 matrix to save all cluster index for each data\n",
    "    \n",
    "    for iter_i in range(10):\n",
    "        W = winit * np.random.rand(clusters_num,n)        # Weight matrix (rows = output neurons, cols = input neurons)\n",
    "        normW = np.sqrt(np.diag(W.dot(W.T)))\n",
    "        normW = normW.reshape(clusters_num,-1)            # reshape normW into a numpy 2d array\n",
    "        W = W / normW                               # normalise using numpy broadcasting\n",
    "\n",
    "        normTrain = np.sqrt(np.diag(np.dot(train.T, train)))\n",
    "        normTrain = normTrain.reshape(m, -1)\n",
    "        train = train/normTrain.T\n",
    "\n",
    "        counter = np.zeros((1,clusters_num))              # counter for the winner neurons\n",
    "        wCount = np.ones((1,tmax+1)) * 0.25         # running avg of the weight change over time\n",
    "        alpha = 0.999\n",
    "\n",
    "        yl = int(round(clusters_num/5))                   # counter for the rows of the subplot\n",
    "        if clusters_num % 5 != 0:\n",
    "            yl += 1\n",
    "\n",
    "        for t in range(1,tmax):\n",
    "            i = math.ceil(m * np.random.rand())-1   # get a randomly generated index in the input range\n",
    "            x = train[:,i]                          # pick a training instance using the random index\n",
    "\n",
    "            h = W.dot(x)/clusters_num                     # get output firing\n",
    "            h = h.reshape((h.shape[0],-1))          # reshape h into a numpy 2d array\n",
    "\n",
    "            xi = np.random.rand(clusters_num,1) / 200\n",
    "            k = np.argmax(h+xi)                     # get the index of the firing neuron\n",
    "\n",
    "            counter[0,k] += 1                       # increment counter for winner neuron\n",
    "\n",
    "            dw = eta * (x.T - W[k,:])               # calculate the change in weights for the k-th output neuron\n",
    "\n",
    "            wCount[0,t] = wCount[0,t-1] * (alpha + dw.dot(dw.T)*(1-alpha)) # % weight change over time (running avg)\n",
    "\n",
    "            W[k,:] = W[k,:] + dw                    # weights for k-th output are updated\n",
    "\n",
    "            if wCount[0, t] < 0.0001: # if it learns sufficiently with learning rate below 0.0001\n",
    "                break\n",
    "\n",
    "        ##########################################\n",
    "        ## save means of clusters into the 3X3 centroids matrix\n",
    "        centroids[:, :, iter_i] = W\n",
    "\n",
    "        ##########################################\n",
    "        # cluster_idx : 2X2 matrix to save all cluster index for each data\n",
    "        ##########################################\n",
    "        ## get indices of clusters for each data\n",
    "        for data in range(m): # check all data\n",
    "            min_vals = []\n",
    "            for weight in range(clusters_num): # compare all clusters for a data\n",
    "                diff = W[weight, :] - train[:, data] # get difference between data and clusters\n",
    "                min_vals.append(sum(np.square(diff))) # get square distance\n",
    "\n",
    "            min_idx = np.argmin(min_vals) # get cluster index which has minimum distance\n",
    "            cluster_idx[iter_i, data] = min_idx # assign the index to the cluster_idx array\n",
    "\n",
    "        # assign True and False if data belongs to specific cluster.\n",
    "        for i in range(clusters_num):\n",
    "            cluster_map = cluster_idx[iter_i, :] == i\n",
    "\n",
    "            # make True and False matrix (cluster) for mapping\n",
    "            cluster = cluster_map\n",
    "            for j in range(14):\n",
    "                cluster = np.vstack((cluster, cluster_map))\n",
    "            #print(np.shape(train[cluster].reshape(n, -1)))\n",
    "\n",
    "            for k in range(clusters_num):\n",
    "                clt = \"cluster{0}\".format(i+1)\n",
    "                globals()[clt]=train[cluster].reshape(n, -1)\n",
    "            #print(\"cluster{0}: \".format(i), np.shape(eval(\"cluster{0}\".format(i))))\n",
    "\n",
    "        ##########################################\n",
    "        ## Get average cost function of 10 iterations\n",
    "        diff_sum = [] # all differences of clusters\n",
    "        for c in range(clusters_num):\n",
    "            diff_vals = [] # difference for one cluster\n",
    "            clt = \"cluster{0}\".format(c+1)\n",
    "            [n2, m2] = np.shape(eval(clt))\n",
    "            for data in range(m2):\n",
    "                diff = np.square(eval(clt)[:, data] - W[c, :])\n",
    "                diff_vals.append(np.sum(diff))\n",
    "\n",
    "            diff_sum.append(np.sum(diff_vals))\n",
    "\n",
    "        costs.append(np.sum(diff_sum))\n",
    "\n",
    "    return min(costs), centroids, cluster_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering checking cost with AIC & BIC & Elbow methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1\n",
      "iteration  2\n",
      "iteration  3\n",
      "iteration  4\n",
      "iteration  5\n",
      "iteration  6\n",
      "iteration  7\n",
      "iteration  8\n",
      "iteration  9\n",
      "iteration  10\n",
      "\n",
      " - process terminal (Run time:41.137846)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import DC_Pickle as dcp\n",
    "t0 = time.clock() # initial time\n",
    "\n",
    "dcp.make_folders(\"../../data/pickles/clusters/\")\n",
    "\n",
    "elbow = []\n",
    "AIC = []\n",
    "BIC = []\n",
    "\n",
    "for clt in range(clusters):\n",
    "    set_clt_filename = \"../../data/pickles/clusters/centroid{0}.pickle\".format(clt+1)\n",
    "    set_idx_filename = \"../../data/pickles/clusters/index{0}.pickle\".format(clt+1)\n",
    "    \n",
    "    [cost, cent, idx] = get_Cost(clt+1, train_data) # return cost, centroid matrix, index matrix\n",
    "    \n",
    "    elbow.append(cost)\n",
    "    AIC.append(cost + 2*clt)\n",
    "    BIC.append(cost +  clt*math.log10(15))\n",
    "    \n",
    "    dcp.make_Pickle(cent, set_clt_filename, force=True)\n",
    "    dcp.make_Pickle(idx, set_idx_filename, force=True)\n",
    "    print(\"iteration \", clt+1)\n",
    "\n",
    "print(\"\\n - process terminal (Run time:{0})\".format(time.clock()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))                                                               \n",
    "ax = fig.add_subplot(1,1,1) \n",
    "\n",
    "ax.grid(which='both')\n",
    "                                   \n",
    "ax.grid(which='major', alpha=0.5)\n",
    "\n",
    "ax.plot(np.arange(clusters)+1, elbow, 'g-', label=\"Elbow\")\n",
    "ax.plot(np.arange(clusters)+1, AIC, 'r-', label=\"AIC\")\n",
    "ax.plot(np.arange(clusters)+1, BIC, 'b-', label=\"BIC\")\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('average cost')\n",
    "ax.legend()\n",
    "fig.savefig('Figs/AIC_BIC_Elbow.png', dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
