{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using competitive learning (without dead unit handling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 300 attempts (with dead unit handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math\n",
    "import DC_Pickle as dcp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_scores = dcp.open_Pickle('../../data/pickles/seperate_origin/eventValue.pickle')\n",
    "train_scores = np.nan_to_num(train_scores[:300, :])\n",
    "\n",
    "[n,m]  = np.shape(train_scores)               # dimension and number of training data\n",
    "\n",
    "eta    = 0.05                               # learning rate\n",
    "winit  = 1                                  # initialise weight\n",
    "\n",
    "tmax   = 40000                              # max learning iteration\n",
    "\n",
    "print(\" - Dimension: {0}\\n - Individuals: {1}\".format(n, m))\n",
    "\n",
    "fig_stats, axes_stats = plt.subplots(2,1)   # fig for the learning stats\n",
    "\n",
    "## function for clustering with cost function return\n",
    "def get_Cost(clusters_num, train):\n",
    "    costs = []\n",
    "    centroids = np.zeros((clusters_num, n, 10), dtype=float)\n",
    "    cluster_idx = np.ones((10, m))*np.nan # 2X2 matrix to save all cluster index for each data\n",
    "    \n",
    "    for iter_i in range(10):\n",
    "        ## dead unit handling by adding noise to inital weights\n",
    "        W = np.ones((clusters_num, n))\n",
    "        for i in range(clusters_num):\n",
    "            index = np.random.choice(m)\n",
    "            W[i] = train_scores[:, index]        # Weight matrix (rows = output neurons, cols = input neurons)\n",
    "        \n",
    "        noise_to_W = np.random.rand(clusters_num, n) / 10       # average weight is 0.05\n",
    "        W = W + noise_to_W\n",
    "        \n",
    "        normW = np.sqrt(np.diag(W.dot(W.T)))\n",
    "        normW = normW.reshape(clusters_num,-1)            # reshape normW into a numpy 2d array\n",
    "        W = W / normW                               # normalise using numpy broadcasting\n",
    "\n",
    "        normTrain = np.sqrt(np.diag(np.dot(train.T, train)))\n",
    "        normTrain = normTrain.reshape(m, -1)\n",
    "        train = train/normTrain.T\n",
    "\n",
    "        counter = np.zeros((1,clusters_num))              # counter for the winner neurons\n",
    "        wCount = np.ones((1,tmax+1)) * 0.25         # running avg of the weight change over time\n",
    "        alpha = 0.999\n",
    "\n",
    "        for t in range(1,tmax):\n",
    "            i = math.ceil(m * np.random.rand())-1   # get a randomly generated index in the input range\n",
    "            x = train[:,i]                          # pick a training instance using the random index\n",
    "\n",
    "            h = W.dot(x)/clusters_num                     # get output firing\n",
    "            h = h.reshape((h.shape[0],-1))          # reshape h into a numpy 2d array\n",
    "\n",
    "            ## add noise to output\n",
    "            # xi = np.random.rand(clusters_num,1) / 200\n",
    "            # k = np.argmax(h+xi)                     # get the index of the firing neuron\n",
    "            k = np.argmax(h)                     # get the index of the firing neuron\n",
    "\n",
    "            counter[0,k] += 1                       # increment counter for winner neuron\n",
    "\n",
    "            dw = eta * (x.T - W[k,:])               # calculate the change in weights for the k-th output neuron\n",
    "\n",
    "            wCount[0,t] = wCount[0,t-1] * (alpha + dw.dot(dw.T)*(1-alpha)) # % weight change over time (running avg)\n",
    "\n",
    "            W[k,:] = W[k,:] + dw                    # weights for k-th output are updated\n",
    "\n",
    "            if wCount[0, t] < 0.0001: # if it learns sufficiently with learning rate below 0.0001\n",
    "                break\n",
    "\n",
    "        ##########################################\n",
    "        ## save means of clusters into the 3X3 centroids matrix\n",
    "        centroids[:, :, iter_i] = W\n",
    "\n",
    "        ##########################################\n",
    "        # cluster_idx : 2X2 matrix to save all cluster index for each data\n",
    "        ##########################################\n",
    "        ## get indices of clusters for each data\n",
    "        for data in range(m): # check all data\n",
    "            min_vals = []\n",
    "            for weight in range(clusters_num): # compare all clusters for a data\n",
    "                diff = W[weight, :] - train[:, data] # get difference between data and clusters\n",
    "                min_vals.append(sum(np.square(diff))) # get square distance\n",
    "\n",
    "            min_idx = np.argmin(min_vals) # get cluster index which has minimum distance\n",
    "            cluster_idx[iter_i, data] = min_idx # assign the index to the cluster_idx array\n",
    "\n",
    "        # assign True and False if data belongs to specific cluster.\n",
    "        for i in range(clusters_num):\n",
    "            cluster_map = cluster_idx[iter_i, :] == i\n",
    "\n",
    "            # make True and False matrix (cluster) for mapping\n",
    "            cluster = cluster_map\n",
    "            for j in range(299):\n",
    "                cluster = np.vstack((cluster, cluster_map))\n",
    "            #print(np.shape(train[cluster].reshape(n, -1)))\n",
    "\n",
    "            for k in range(clusters_num):\n",
    "                clt = \"cluster{0}\".format(i+1)\n",
    "                globals()[clt]=train[cluster].reshape(n, -1)\n",
    "            #print(\"cluster{0}: \".format(i), np.shape(eval(\"cluster{0}\".format(i))))\n",
    "\n",
    "        ##########################################\n",
    "        ## Get average cost function of 10 iterations\n",
    "        diff_sum = [] # all differences of clusters\n",
    "        for c in range(clusters_num):\n",
    "            diff_vals = [] # difference for one cluster\n",
    "            clt = \"cluster{0}\".format(c+1)\n",
    "            [n2, m2] = np.shape(eval(clt))\n",
    "            for data in range(m2):\n",
    "                diff = np.square(eval(clt)[:, data] - W[c, :])\n",
    "                diff_vals.append(np.sum(diff))\n",
    "\n",
    "            diff_sum.append(np.sum(diff_vals))\n",
    "\n",
    "        costs.append(np.sum(diff_sum))\n",
    "        \n",
    "        # average of weight changes in time\n",
    "        axes_stats[0].clear()\n",
    "        axes_stats[0].set_title(\"Weight change in time\")\n",
    "        axes_stats[0].semilogx(wCount[0, 2:t + 1], '-b', linewidth=2.0)     # range of 2:t+1 is 3 ~ t+1\n",
    "        axes_stats[0].set_xlim([1, 3*10000])\n",
    "        axes_stats[0].set_ylim([-0.001, 0.255])\n",
    "        \n",
    "        # plot the number of output neurons' firing\n",
    "        axes_stats[1].clear()\n",
    "        axes_stats[1].set_title(\"Ouput neurons' entire firing number\")\n",
    "        axes_stats[1].bar(np.arange(1, clusters_num+1), counter.T, align='center')\n",
    "        # axes_stats[4].set_xticks(np.arange(1, digits + 1))              # set the x axis numbering\n",
    "        axes_stats[1].relim()                                           # recompute axes_stats' data limit\n",
    "        axes_stats[1].autoscale_view(True, True, True)                  # update axes_stats' axis using the new data limit\n",
    "        \n",
    "        #plt.tight_layout(pad=0.5)\n",
    "        dcp.make_folders('../../Figs/Learning_State/origin_set/300attempts/cluster{0}/'.format(clusters_num))\n",
    "        fig_stats.savefig('../../Figs/Learning_State/origin_set/300attempts/cluster{0}/iter{1}.png'.format(clusters_num, iter_i), dpi=100)\n",
    "\n",
    "    return min(costs), centroids, cluster_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import DC_Pickle as dcp\n",
    "t0 = time.clock() # initial time\n",
    "\n",
    "dcp.make_folders(\"../../data/pickles/clusters_origin/300attempts/\")\n",
    "\n",
    "nClusters = 5\n",
    "elbow = []\n",
    "AIC = []\n",
    "BIC = []\n",
    "\n",
    "for clt in range(nClusters):\n",
    "    set_clt_filename = \"../../data/pickles/clusters_origin/300attempts/centroid{0}.pickle\".format(clt+1)\n",
    "    set_idx_filename = \"../../data/pickles/clusters_origin/300attempts/index{0}.pickle\".format(clt+1)\n",
    "    # get_Cost(clt+1, train_scores) # return cost, centroid matrix, index matrix\n",
    "\n",
    "    [cost, cent, idx] = get_Cost(clt+1, train_scores) # return cost, centroid matrix, index matrix\n",
    "    \n",
    "    elbow.append(cost)\n",
    "    AIC.append(cost + 2*clt*nClusters)\n",
    "    BIC.append(cost +  clt*math.log10(300)*nClusters)\n",
    "    \n",
    "    dcp.make_Pickle(cent, set_clt_filename, force=True)\n",
    "    dcp.make_Pickle(idx, set_idx_filename, force=True)\n",
    "    print(\"iteration \", clt+1)\n",
    "\n",
    "print(\"\\n - process terminal (Run time:{0})\".format(time.clock()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))                                                               \n",
    "ax = fig.add_subplot(1,1,1) \n",
    "\n",
    "ax.grid(which='both')\n",
    "                                   \n",
    "ax.grid(which='major', alpha=0.5)\n",
    "\n",
    "ax.plot(np.arange(nClusters)+1, elbow, 'g-', label=\"Elbow\")\n",
    "ax.plot(np.arange(nClusters)+1, AIC, 'r-', label=\"AIC\")\n",
    "ax.plot(np.arange(nClusters)+1, BIC, 'b-', label=\"BIC\")\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('average cost')\n",
    "ax.legend()\n",
    "#fig.savefig('../../Figs/AIC_BIC_Elbow.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import DC_Pickle as dcp\n",
    "%matplotlib inline\n",
    "\n",
    "dcp.make_folders(\"../../Figs/Centroids/Test/300Attempts/\")\n",
    "clt_num = 8\n",
    "attempts = np.arange(300)+1\n",
    "\n",
    "centroids = dcp.open_Pickle(\"../../data/pickles/clusters_test/300attempts/centroids/centroid10.pickle\")\n",
    "indices = dcp.open_Pickle(\"../../data/pickles/clusters_test/300attempts/indices/index10.pickle\")\n",
    "\n",
    "centroids = centroids[:, :, 6]\n",
    "\n",
    "for i in range(clt_num):\n",
    "    idx = indices[6] == i\n",
    "    ply = train_scores[:, idx]*10e-6\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))   # fig for the learning stats\n",
    "    ax.plot(attempts, centroids[i, :], 'o', c='black')\n",
    "    #ax.plot(attempts, ply, 'x', c='b')\n",
    "    ax.set_title(\"Centroid of cluster {0}\".format(i+1), fontsize=16)\n",
    "    plt.xlabel('attempt', fontsize=14)\n",
    "    plt.ylabel('score', fontsize=14)\n",
    "    \n",
    "    fig.savefig(\"../../Figs/Centroids/Test/300Attempts/{0}\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math\n",
    "import DC_Pickle as dcp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_scores = dcp.open_Pickle('../../data/pickles/seperate_test/eventValue.pickle')\n",
    "train_scores = train_scores[:15, :]\n",
    "\n",
    "train_data = np.ones(15)*np.nan\n",
    "idx_data = []\n",
    "\n",
    "for i in range(train_scores.shape[1]):\n",
    "    if not np.isnan(train_scores[:, i]).any():\n",
    "        train_data = np.vstack((train_data, train_scores[:, i]))\n",
    "        idx_data.append(i)\n",
    "        \n",
    "train_data = np.delete(train_data, (0), axis=0) # delete the first row including nan\n",
    "train_data = train_data.T\n",
    "\n",
    "[n,m]  = np.shape(train_data)               # dimension and number of training data\n",
    "\n",
    "eta    = 0.05                               # learning rate\n",
    "winit  = 1                                  # initialise weight\n",
    "\n",
    "tmax   = 40000                              # max learning iteration\n",
    "\n",
    "print(\" - Dimension: {0}\\n - Individuals: {1}\".format(n, m))\n",
    "\n",
    "fig_stats, axes_stats = plt.subplots(2,1)   # fig for the learning stats\n",
    "\n",
    "## function for clustering with cost function return\n",
    "def get_Cost(clusters_num, train):\n",
    "    costs = []\n",
    "    centroids = np.zeros((clusters_num, n, 10), dtype=float)\n",
    "    cluster_idx = np.ones((10, m))*np.nan # 2X2 matrix to save all cluster index for each data\n",
    "    \n",
    "    for iter_i in range(10):\n",
    "        W = winit * np.random.rand(clusters_num,n)        # Weight matrix (rows = output neurons, cols = input neurons)\n",
    "        normW = np.sqrt(np.diag(W.dot(W.T)))\n",
    "        normW = normW.reshape(clusters_num,-1)            # reshape normW into a numpy 2d array\n",
    "        W = W / normW                               # normalise using numpy broadcasting\n",
    "\n",
    "        normTrain = np.sqrt(np.diag(np.dot(train.T, train)))\n",
    "        normTrain = normTrain.reshape(m, -1)\n",
    "        train = train/normTrain.T\n",
    "\n",
    "        counter = np.zeros((1,clusters_num))              # counter for the winner neurons\n",
    "        wCount = np.ones((1,tmax+1)) * 0.25         # running avg of the weight change over time\n",
    "        alpha = 0.999\n",
    "\n",
    "        yl = int(round(clusters_num/5))                   # counter for the rows of the subplot\n",
    "        if clusters_num % 5 != 0:\n",
    "            yl += 1\n",
    "\n",
    "        for t in range(1,tmax):\n",
    "            i = math.ceil(m * np.random.rand())-1   # get a randomly generated index in the input range\n",
    "            x = train[:,i]                          # pick a training instance using the random index\n",
    "\n",
    "            h = W.dot(x)/clusters_num                     # get output firing\n",
    "            h = h.reshape((h.shape[0],-1))          # reshape h into a numpy 2d array\n",
    "            k = np.argmax(h)                     # get the index of the firing neuron\n",
    "\n",
    "            counter[0,k] += 1                       # increment counter for winner neuron\n",
    "\n",
    "            dw = eta * (x.T - W[k,:])               # calculate the change in weights for the k-th output neuron\n",
    "\n",
    "            wCount[0,t] = wCount[0,t-1] * (alpha + dw.dot(dw.T)*(1-alpha)) # % weight change over time (running avg)\n",
    "\n",
    "            W[k,:] = W[k,:] + dw                    # weights for k-th output are updated\n",
    "\n",
    "            if wCount[0, t] < 0.0001: # if it learns sufficiently with learning rate below 0.0001\n",
    "                break\n",
    "\n",
    "        ##########################################\n",
    "        ## save means of clusters into the 3X3 centroids matrix\n",
    "        centroids[:, :, iter_i] = W\n",
    "\n",
    "        ##########################################\n",
    "        # cluster_idx : 2X2 matrix to save all cluster index for each data\n",
    "        ##########################################\n",
    "        ## get indices of clusters for each data\n",
    "        for data in range(m): # check all data\n",
    "            min_vals = []\n",
    "            for weight in range(clusters_num): # compare all clusters for a data\n",
    "                diff = W[weight, :] - train[:, data] # get difference between data and clusters\n",
    "                min_vals.append(sum(np.square(diff))) # get square distance\n",
    "\n",
    "            min_idx = np.argmin(min_vals) # get cluster index which has minimum distance\n",
    "            cluster_idx[iter_i, data] = min_idx # assign the index to the cluster_idx array\n",
    "\n",
    "        # assign True and False if data belongs to specific cluster.\n",
    "        for i in range(clusters_num):\n",
    "            cluster_map = cluster_idx[iter_i, :] == i\n",
    "\n",
    "            # make True and False matrix (cluster) for mapping\n",
    "            cluster = cluster_map\n",
    "            for j in range(14):\n",
    "                cluster = np.vstack((cluster, cluster_map))\n",
    "            #print(np.shape(train[cluster].reshape(n, -1)))\n",
    "\n",
    "            for k in range(clusters_num):\n",
    "                clt = \"cluster{0}\".format(i+1)\n",
    "                globals()[clt]=train[cluster].reshape(n, -1)\n",
    "            #print(\"cluster{0}: \".format(i), np.shape(eval(\"cluster{0}\".format(i))))\n",
    "\n",
    "        ##########################################\n",
    "        ## Get average cost function of 10 iterations\n",
    "        diff_sum = [] # all differences of clusters\n",
    "        for c in range(clusters_num):\n",
    "            diff_vals = [] # difference for one cluster\n",
    "            clt = \"cluster{0}\".format(c+1)\n",
    "            [n2, m2] = np.shape(eval(clt))\n",
    "            for data in range(m2):\n",
    "                diff = np.square(eval(clt)[:, data] - W[c, :])\n",
    "                diff_vals.append(np.sum(diff))\n",
    "\n",
    "            diff_sum.append(np.sum(diff_vals))\n",
    "\n",
    "        costs.append(np.sum(diff_sum))\n",
    "        \n",
    "        # average of weight changes in time\n",
    "        axes_stats[0].clear()\n",
    "        axes_stats[0].set_title(\"Weight change in time\")\n",
    "        axes_stats[0].semilogx(wCount[0, 2:t + 1], '-b', linewidth=2.0)     # range of 2:t+1 is 3 ~ t+1\n",
    "        axes_stats[0].set_xlim([1, 4*10000])\n",
    "        axes_stats[0].set_ylim([-0.001, 0.255])\n",
    "        \n",
    "        # plot the number of output neurons' firing\n",
    "        axes_stats[1].clear()\n",
    "        axes_stats[1].set_title(\"Ouput neurons' entire firing number\")\n",
    "        axes_stats[1].bar(np.arange(1, clusters_num+1), counter.T, align='center')\n",
    "        # axes_stats[4].set_xticks(np.arange(1, digits + 1))              # set the x axis numbering\n",
    "        axes_stats[1].relim()                                           # recompute axes_stats' data limit\n",
    "        axes_stats[1].autoscale_view(True, True, True)                  # update axes_stats' axis using the new data limit\n",
    "        \n",
    "        #plt.tight_layout(pad=0.5)\n",
    "        dcp.make_folders('../../Figs/Learning_State/test_set/300attempts/cluster{0}/'.format(clusters_num))\n",
    "        fig_stats.savefig('../../Figs/Learning_State/test_set/300attempts/cluster{0}/iter{1}.png'.format(clusters_num, iter_i), dpi=100)\n",
    "\n",
    "    return min(costs), centroids, cluster_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using competitive learning (with dead unit handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import math\n",
    "import DC_Pickle as dcp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_scores = dcp.open_Pickle('../../data/pickles/seperate_origin/eventValue.pickle')\n",
    "train_scores = train_scores[:15, :]\n",
    "\n",
    "train_data = np.ones(15)*np.nan\n",
    "idx_data = []\n",
    "\n",
    "for i in range(train_scores.shape[1]):\n",
    "    if not np.isnan(train_scores[:, i]).any():\n",
    "        train_data = np.vstack((train_data, train_scores[:, i]))\n",
    "        idx_data.append(i)\n",
    "        \n",
    "train_data = np.delete(train_data, (0), axis=0) # delete the first row including nan\n",
    "train_data = train_data.T\n",
    "\n",
    "[n,m]  = np.shape(train_data)               # dimension and number of training data\n",
    "\n",
    "eta    = 0.05                               # learning rate\n",
    "winit  = 1                                  # initialise weight\n",
    "\n",
    "tmax   = 40000                              # max learning iteration\n",
    "\n",
    "print(\" - Dimension: {0}\\n - Individuals: {1}\".format(n, m))\n",
    "\n",
    "fig_stats, axes_stats = plt.subplots(2,1)   # fig for the learning stats\n",
    "\n",
    "## function for clustering with cost function return\n",
    "def get_Cost(clusters_num, train):\n",
    "    costs = []\n",
    "    centroids = np.zeros((clusters_num, n, 10), dtype=float)\n",
    "    cluster_idx = np.ones((10, m))*np.nan # 2X2 matrix to save all cluster index for each data\n",
    "    \n",
    "    for iter_i in range(10):\n",
    "        ## dead unit handling by adding noise to inital weights\n",
    "        W = np.ones((clusters_num, n))\n",
    "        for i in range(clusters_num):\n",
    "            index = np.random.choice(m)\n",
    "            W[i] = train_data[:, index]        # Weight matrix (rows = output neurons, cols = input neurons)\n",
    "        \n",
    "        noise_to_W = np.random.rand(clusters_num, n) / 10       # average weight is 0.05\n",
    "        W = W + noise_to_W\n",
    "        \n",
    "        normW = np.sqrt(np.diag(W.dot(W.T)))\n",
    "        normW = normW.reshape(clusters_num,-1)            # reshape normW into a numpy 2d array\n",
    "        W = W / normW                               # normalise using numpy broadcasting\n",
    "\n",
    "        normTrain = np.sqrt(np.diag(np.dot(train.T, train)))\n",
    "        normTrain = normTrain.reshape(m, -1)\n",
    "        train = train/normTrain.T\n",
    "\n",
    "        counter = np.zeros((1,clusters_num))              # counter for the winner neurons\n",
    "        wCount = np.ones((1,tmax+1)) * 0.25         # running avg of the weight change over time\n",
    "        alpha = 0.999\n",
    "\n",
    "        yl = int(round(clusters_num/5))                   # counter for the rows of the subplot\n",
    "        if clusters_num % 5 != 0:\n",
    "            yl += 1\n",
    "\n",
    "        for t in range(1,tmax):\n",
    "            i = math.ceil(m * np.random.rand())-1   # get a randomly generated index in the input range\n",
    "            x = train[:,i]                          # pick a training instance using the random index\n",
    "\n",
    "            h = W.dot(x)/clusters_num                     # get output firing\n",
    "            h = h.reshape((h.shape[0],-1))          # reshape h into a numpy 2d array\n",
    "\n",
    "            ## add noise to output\n",
    "            # xi = np.random.rand(clusters_num,1) / 200\n",
    "            # k = np.argmax(h+xi)                     # get the index of the firing neuron\n",
    "            k = np.argmax(h)                     # get the index of the firing neuron\n",
    "\n",
    "            counter[0,k] += 1                       # increment counter for winner neuron\n",
    "\n",
    "            dw = eta * (x.T - W[k,:])               # calculate the change in weights for the k-th output neuron\n",
    "\n",
    "            wCount[0,t] = wCount[0,t-1] * (alpha + dw.dot(dw.T)*(1-alpha)) # % weight change over time (running avg)\n",
    "\n",
    "            W[k,:] = W[k,:] + dw                    # weights for k-th output are updated\n",
    "\n",
    "            if wCount[0, t] < 0.0001: # if it learns sufficiently with learning rate below 0.0001\n",
    "                break\n",
    "\n",
    "        ##########################################\n",
    "        ## save means of clusters into the 3X3 centroids matrix\n",
    "        centroids[:, :, iter_i] = W\n",
    "\n",
    "        ##########################################\n",
    "        # cluster_idx : 2X2 matrix to save all cluster index for each data\n",
    "        ##########################################\n",
    "        ## get indices of clusters for each data\n",
    "        for data in range(m): # check all data\n",
    "            min_vals = []\n",
    "            for weight in range(clusters_num): # compare all clusters for a data\n",
    "                diff = W[weight, :] - train[:, data] # get difference between data and clusters\n",
    "                min_vals.append(sum(np.square(diff))) # get square distance\n",
    "\n",
    "            min_idx = np.argmin(min_vals) # get cluster index which has minimum distance\n",
    "            cluster_idx[iter_i, data] = min_idx # assign the index to the cluster_idx array\n",
    "\n",
    "        # assign True and False if data belongs to specific cluster.\n",
    "        for i in range(clusters_num):\n",
    "            cluster_map = cluster_idx[iter_i, :] == i\n",
    "\n",
    "            # make True and False matrix (cluster) for mapping\n",
    "            cluster = cluster_map\n",
    "            for j in range(14):\n",
    "                cluster = np.vstack((cluster, cluster_map))\n",
    "            #print(np.shape(train[cluster].reshape(n, -1)))\n",
    "\n",
    "            for k in range(clusters_num):\n",
    "                clt = \"cluster{0}\".format(i+1)\n",
    "                globals()[clt]=train[cluster].reshape(n, -1)\n",
    "            #print(\"cluster{0}: \".format(i), np.shape(eval(\"cluster{0}\".format(i))))\n",
    "\n",
    "        ##########################################\n",
    "        ## Get average cost function of 10 iterations\n",
    "        diff_sum = [] # all differences of clusters\n",
    "        for c in range(clusters_num):\n",
    "            diff_vals = [] # difference for one cluster\n",
    "            clt = \"cluster{0}\".format(c+1)\n",
    "            [n2, m2] = np.shape(eval(clt))\n",
    "            for data in range(m2):\n",
    "                diff = np.square(eval(clt)[:, data] - W[c, :])\n",
    "                diff_vals.append(np.sum(diff))\n",
    "\n",
    "            diff_sum.append(np.sum(diff_vals))\n",
    "\n",
    "        costs.append(np.sum(diff_sum))\n",
    "        \n",
    "        # average of weight changes in time\n",
    "        axes_stats[0].clear()\n",
    "        axes_stats[0].set_title(\"Weight change in time\")\n",
    "        axes_stats[0].semilogx(wCount[0, 2:t + 1], '-b', linewidth=2.0)     # range of 2:t+1 is 3 ~ t+1\n",
    "        axes_stats[0].set_xlim([1, 3*10000])\n",
    "        axes_stats[0].set_ylim([-0.001, 0.255])\n",
    "        \n",
    "        # plot the number of output neurons' firing\n",
    "        axes_stats[1].clear()\n",
    "        axes_stats[1].set_title(\"Ouput neurons' entire firing number\")\n",
    "        axes_stats[1].bar(np.arange(1, clusters_num+1), counter.T, align='center')\n",
    "        # axes_stats[4].set_xticks(np.arange(1, digits + 1))              # set the x axis numbering\n",
    "        axes_stats[1].relim()                                           # recompute axes_stats' data limit\n",
    "        axes_stats[1].autoscale_view(True, True, True)                  # update axes_stats' axis using the new data limit\n",
    "        \n",
    "        #plt.tight_layout(pad=0.5)\n",
    "        dcp.make_folders('../../Figs/Learning_State/origin_set/cluster{0}/'.format(clusters_num))\n",
    "        fig_stats.savefig('../../Figs/Learning_State/origin_set/cluster{0}/iter{1}.png'.format(clusters_num, iter_i), dpi=100)\n",
    "\n",
    "    return min(costs), centroids, cluster_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering checking cost with AIC & BIC & Elbow methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import DC_Pickle as dcp\n",
    "t0 = time.clock() # initial time\n",
    "\n",
    "dcp.make_folders(\"../../data/pickles/clusters_test/\")\n",
    "\n",
    "dimension = 10\n",
    "elbow = []\n",
    "AIC = []\n",
    "BIC = []\n",
    "\n",
    "for clt in range(dimension):\n",
    "    set_clt_filename = \"../../data/pickles/clusters_test/centroid{0}.pickle\".format(clt+1)\n",
    "    set_idx_filename = \"../../data/pickles/clusters_test/index{0}.pickle\".format(clt+1)\n",
    "    \n",
    "    [cost, cent, idx] = get_Cost(clt+1, train_data) # return cost, centroid matrix, index matrix\n",
    "    \n",
    "    elbow.append(cost)\n",
    "    AIC.append(cost + 2*clt*dimension)\n",
    "    BIC.append(cost +  clt*math.log10(15)*dimension)\n",
    "    \n",
    "    #dcp.make_Pickle(cent, set_clt_filename, force=True)\n",
    "    #dcp.make_Pickle(idx, set_idx_filename, force=True)\n",
    "    print(\"iteration \", clt+1)\n",
    "\n",
    "print(\"\\n - process terminal (Run time:{0})\".format(time.clock()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcp.make_folders(\"../../data/pickles/clusters_test/300attempts/costs/\")\n",
    "dcp.make_Pickle(elbow, \"../../data/pickles/clusters_test/300attempts/costs/elbow.pickle\")\n",
    "dcp.make_Pickle(AIC, \"../../data/pickles/clusters_test/300attempts/costs/AIC.pickle\")\n",
    "dcp.make_Pickle(BIC, \"../../data/pickles/clusters_test/300attempts/costs/BIC.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))                                                               \n",
    "ax = fig.add_subplot(1,1,1) \n",
    "\n",
    "ax.grid(which='both')\n",
    "                                   \n",
    "ax.grid(which='major', alpha=0.5)\n",
    "\n",
    "ax.plot(np.arange(nClusters)+1, elbow, 'g-', label=\"Elbow\")\n",
    "ax.plot(np.arange(nClusters)+1, AIC, 'r-', label=\"AIC\")\n",
    "ax.plot(np.arange(nClusters)+1, BIC, 'b-', label=\"BIC\")\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('average cost')\n",
    "ax.legend()\n",
    "#fig.savefig('../../Figs/AIC_BIC_Elbow.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import DC_Pickle as dcp\n",
    "%matplotlib inline\n",
    "\n",
    "dcp.make_folders(\"../../Figs/Centroids/Test/300Attempts/\")\n",
    "clt_num = 8\n",
    "attempts = np.arange(300)+1\n",
    "\n",
    "centroids = dcp.open_Pickle(\"../../data/pickles/clusters_test/300attempts/centroids/centroid10.pickle\")\n",
    "indices = dcp.open_Pickle(\"../../data/pickles/clusters_test/300attempts/indices/index10.pickle\")\n",
    "\n",
    "centroids = centroids[:, :, 6]\n",
    "\n",
    "for i in range(clt_num):\n",
    "    idx = indices[6] == i\n",
    "    ply = train_scores[:, idx]*10e-6\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))   # fig for the learning stats\n",
    "    ax.plot(attempts, centroids[i, :], 'o', c='black')\n",
    "    ax.plot(attempts, ply, 'x', c='b')\n",
    "    ax.set_title(\"Centroid of cluster {0}\".format(i+1), fontsize=16)\n",
    "    plt.xlabel('attempt', fontsize=14)\n",
    "    plt.ylabel('score', fontsize=14)\n",
    "    \n",
    "    #fig.savefig(\"../../Figs/Centroids/Origin/{0}\".format(i+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
