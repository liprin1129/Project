\relax 
\citation{stuart2009Axon}
\citation{newell1981mechanisms}
\citation{lacroix2006introduction}
\citation{newell1981mechanisms}
\citation{heathcote2000power}
\citation{gallistel2004learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background and Motivation}{1}}
\citation{stafford2014tracing}
\citation{stafford2014tracing}
\citation{donner2015piecewise}
\citation{gallistel2004learning}
\citation{kounios2009aha}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Definition}{2}}
\citation{gaschler2007playing}
\citation{murre2011power}
\citation{stafford2014tracing}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Aims and Objectives}{3}}
\citation{murre2011power}
\citation{gaschler2007playing}
\citation{gallistel2004learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Project Management}{4}}
\citation{stuart2009Axon}
\citation{stonebraker2013data}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Data Curation}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{stafford2014tracing}
\citation{stafford2016testing}
\citation{stafford2014tracing}
\citation{stafford2016testing}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data Cleaning}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Data accommodating}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Original variables in the data source}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Hidden variables calculated from the original variables}{7}}
\citation{stafford2016testing}
\citation{shafranovich2008common}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Data Deformation}{8}}
\citation{idris2014python}
\citation{idris2014python}
\citation{python2016pickle}
\citation{python2016pickle}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Pickle file format}{9}}
\citation{python2016multiprocessing}
\citation{newell1981mechanisms}
\citation{gallistel2004learning}
\citation{donner2015piecewise}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Data storage and index table}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Multiprocessing}{10}}
\citation{donner2015piecewise}
\citation{hackeling2014mastering}
\citation{harrington2012machine}
\citation{daume2012course}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}K-Means Clustering with Competitive Learning Algorithm}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}K-Means clustering}{12}}
\newlabel{eq:threshold1}{{2.1}{12}}
\newlabel{eq:k_means_cost_function}{{2.2}{12}}
\newlabel{eq:threshold2}{{2.3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Competitive learning}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Competitive neuron diagram. When an input enters the network, it generates output values according to weights between an input and output neurons (blue arrows). Output neurons compete with each other (red arrows), then only one output fires on an input pattern. Output neurons are considered only having binary $0/1$ output.\relax }}{14}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Competitive_Diagram}{{2.1}{14}}
\citation{bowles2015machine}
\newlabel{eq:competitive1}{{2.4}{15}}
\newlabel{eq:competitive2}{{2.5}{15}}
\newlabel{eq:competitive3}{{2.6}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Normalisation}{15}}
\newlabel{eq:inner_product}{{2.7}{15}}
\newlabel{fig:not_norm_data}{{2.2a}{16}}
\newlabel{sub@fig:not_norm_data}{{a}{16}}
\newlabel{fig:not_norm_weight}{{2.2b}{16}}
\newlabel{sub@fig:not_norm_weight}{{b}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Examples of not-normalised data and weight in 3 dimensions. In both left and right figures, sphere represents unit length area.\relax }}{16}}
\newlabel{fig:not-normalised}{{2.2}{16}}
\newlabel{eq:distance}{{2.8}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Procedure of the algorithm}{17}}
\citation{hackeling2014mastering}
\newlabel{eq:on-line and batch}{{2.9}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Local optima and optimal number of clusters}{18}}
\newlabel{sbs:number_clusters}{{2.4.5}{18}}
\citation{daume2012course}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison between AIC and BIC according to the different number of k. AIC are more strict method than BIC to identify the number of clusters.\relax }}{19}}
\newlabel{fig:AIC_BIC_Elbow}{{2.3}{19}}
\newlabel{eq:AICandBIC}{{2.10}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Dealing with dead unit}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces An example of dead unit problem. Red and Green arrows are initialised weight before applying into the network. Green arrow is set far from both two input clusters, and it is assumed as a dead unit. Unfortunately, green arrow will never fire through the learning, and which means that input vectors are not properly clustered.\relax }}{21}}
\newlabel{fig:dead_unit}{{2.4}{21}}
\newlabel{eq:leaky_learning}{{2.11}{21}}
\newlabel{eq:add_bias}{{2.12}{22}}
\newlabel{eq:add_noise}{{2.13}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Learning rate and existence of dead units\relax }}{23}}
\newlabel{fig:dead_state}{{2.5}{23}}
\newlabel{eq:SV}{{2.14}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Learning rate and and entire firing number of output neurons after using SV approach for dead units\relax }}{24}}
\newlabel{fig:dead_states_solved}{{2.6}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.7}Centroids}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Centroids of cluster from 1 to 4\relax }}{25}}
\newlabel{fig:centroid1}{{2.7}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Centroids of cluster from 5 to 8\relax }}{25}}
\newlabel{fig:centroid2}{{2.8}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Centroids of cluster from 9 to 12\relax }}{26}}
\newlabel{fig:centroid3}{{2.9}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Centroids of cluster 13 and 14\relax }}{26}}
\newlabel{fig:centroid4}{{2.10}{26}}
\citation{james2013introduction}
\citation{rogers2015first}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Function Fitting}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:unbiased}{{3.1}{27}}
\citation{howard2014learning}
\citation{murre2011power}
\citation{gallistel2004learning}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Functions}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Parametric Method}{28}}
\newlabel{sbs:Parametric Method}{{3.1.1}{28}}
\newlabel{eq:first_order}{{3.2}{29}}
\newlabel{eq:vector_matrix}{{3.3}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Functions for Single Curve}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Possible 4 models; first order polynomial, 3 parameters exponential function, 4 parameters power function, step-like function with change point at 5\relax }}{30}}
\newlabel{fig:Functions}{{3.1}{30}}
\citation{heathcote2000power}
\newlabel{eq:high_order}{{3.4}{31}}
\newlabel{eq:exponential_function}{{3.5}{31}}
\newlabel{eq:power_function}{{3.6}{32}}
\newlabel{eq:step_like}{{3.7}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Piecewise Function}{32}}
\newlabel{sbs:piecewise}{{3.1.3}{32}}
\citation{gallistel2004learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Example of piecewise functions; exponential and power functions\relax }}{33}}
\newlabel{fig:Piecewise_Functions}{{3.2}{33}}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Method}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Squared Lost Function}{34}}
\newlabel{sbs:square_lost_function}{{3.2.1}{34}}
\newlabel{eq:loss_function}{{3.10}{35}}
\newlabel{eq:avg_loss_function}{{3.11}{35}}
\newlabel{eq:loss_derivative}{{3.12}{35}}
\newlabel{eq:loss_zero}{{3.13}{35}}
\citation{pythonLeastSquare}
\citation{rogers2015first}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Training Set and Validation Set}{37}}
\newlabel{sbs:Training_Validation}{{3.2.2}{37}}
\newlabel{fig:train_valid_1_a}{{3.3a}{38}}
\newlabel{sub@fig:train_valid_1_a}{{a}{38}}
\newlabel{fig:train_valid_1_b}{{3.3b}{38}}
\newlabel{sub@fig:train_valid_1_b}{{b}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Minimum cost in training set, but large cost in validation set. Exponential function fitting with 3 parameters to 15 training set of cluster 13, and validation check\relax }}{38}}
\newlabel{fig:train_valid_1}{{3.3}{38}}
\newlabel{fig:train_valid_2_a}{{3.4a}{38}}
\newlabel{sub@fig:train_valid_2_a}{{a}{38}}
\newlabel{fig:train_valid_2_b}{{3.4b}{38}}
\newlabel{sub@fig:train_valid_2_b}{{b}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The learning curve which has the minimum cost in validation set. Exponential function fitting with 3 parameters to 15 training set of cluster 1, and validation check\relax }}{38}}
\newlabel{fig:not-fig:train_valid_2}{{3.4}{38}}
\citation{rogers2015first}
\citation{rogers2015first}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Over-fitting and Regularisation}{39}}
\newlabel{sbs:over-fitting}{{3.2.3}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Regularisation example of cluster 4 in the piecewise power function with 3 parameters.\relax }}{39}}
\newlabel{fig:pieces}{{3.5}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Regularisation example of cluster 5 in the piecewise step-like functions.\relax }}{40}}
\newlabel{fig:pieces}{{3.6}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Choice of Breaking Point}{40}}
\newlabel{abs:breaking_point}{{3.2.4}{40}}
\newlabel{eq:cost_matrix}{{3.15}{41}}
\newlabel{eq:length_matrix}{{3.16}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Transition point shift in 4 pieces\relax }}{42}}
\newlabel{fig:transition_shift}{{3.7}{42}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Discussion}{43}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The Best Learning Curve}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Power Law and Exponential Law}{44}}
\newlabel{sba:PL_EL}{{4.1.1}{44}}
\newlabel{fig:single_exponential3}{{4.1a}{44}}
\newlabel{sub@fig:single_exponential3}{{a}{44}}
\newlabel{fig:single_powerlaw4}{{4.1b}{44}}
\newlabel{sub@fig:single_powerlaw4}{{b}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparison between exponential function and power function. Both are the best fitting learning curve to the entirely averaged data set among different number of parameters.\relax }}{44}}
\newlabel{fig:single_functions}{{4.1}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Single and Piecewise Curves}{45}}
\newlabel{fig:best_fit_5_b}{{4.6b}{48}}
\newlabel{sub@fig:best_fit_5_b}{{b}{48}}
\newlabel{fig:best_fit_6_b}{{4.7b}{48}}
\newlabel{sub@fig:best_fit_6_b}{{b}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The result of the best function fitting curves for each clusters.\relax }}{48}}
\newlabel{fig:best_fit}{{4.8}{48}}
\citation{stafford2014tracing}
\citation{sutton1998reinforcement}
\citation{stafford2014tracing}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Exploitation and Exploration}{49}}
\bibstyle{apacite}
\bibdata{references}
\bibcite{bowles2015machine}{\citeauthoryear {Bowles}{Bowles}{{\APACyear {2015}}}}
\APACbibcite{bowles2015machine}{\citeauthoryear {Bowles}{Bowles}{{\APACyear {2015}}}}
\bibcite{daume2012course}{\citeauthoryear {Daum{\'e}\nobreakspace  {}III}{Daum{\'e}\nobreakspace  {}III}{{\APACyear {2012}}}}
\APACbibcite{daume2012course}{\citeauthoryear {Daum{\'e}\nobreakspace  {}III}{Daum{\'e}\nobreakspace  {}III}{{\APACyear {2012}}}}
\bibcite{donner2015piecewise}{\citeauthoryear {Donner\ \BBA{} Hardy}{Donner\ \BBA{} Hardy}{{\APACyear {2015}}}}
\APACbibcite{donner2015piecewise}{\citeauthoryear {Donner\ \BBA{} Hardy}{Donner\ \BBA{} Hardy}{{\APACyear {2015}}}}
\bibcite{gallistel2004learning}{\citeauthoryear {Gallistel, Fairhurst,{}\ \BBA{} Balsam}{Gallistel\ \BOthers {.}}{{\APACyear {2004}}}}
\APACbibcite{gallistel2004learning}{\citeauthoryear {Gallistel, Fairhurst,{}\ \BBA{} Balsam}{Gallistel\ \BOthers {.}}{{\APACyear {2004}}}}
\bibcite{gaschler2007playing}{\citeauthoryear {Gaschler, Progscha, Smallbone, Ram,{}\ \BBA{} Bilalic}{Gaschler\ \BOthers {.}}{{\APACyear {2007}}}}
\APACbibcite{gaschler2007playing}{\citeauthoryear {Gaschler, Progscha, Smallbone, Ram,{}\ \BBA{} Bilalic}{Gaschler\ \BOthers {.}}{{\APACyear {2007}}}}
\bibcite{hackeling2014mastering}{\citeauthoryear {Hackeling}{Hackeling}{{\APACyear {2014}}}}
\APACbibcite{hackeling2014mastering}{\citeauthoryear {Hackeling}{Hackeling}{{\APACyear {2014}}}}
\bibcite{harrington2012machine}{\citeauthoryear {Harrington}{Harrington}{{\APACyear {2012}}}}
\APACbibcite{harrington2012machine}{\citeauthoryear {Harrington}{Harrington}{{\APACyear {2012}}}}
\bibcite{heathcote2000power}{\citeauthoryear {Heathcote, Brown,{}\ \BBA{} Mewhort}{Heathcote\ \BOthers {.}}{{\APACyear {2000}}}}
\APACbibcite{heathcote2000power}{\citeauthoryear {Heathcote, Brown,{}\ \BBA{} Mewhort}{Heathcote\ \BOthers {.}}{{\APACyear {2000}}}}
\bibcite{howard2014learning}{\citeauthoryear {Howard}{Howard}{{\APACyear {2014}}}}
\APACbibcite{howard2014learning}{\citeauthoryear {Howard}{Howard}{{\APACyear {2014}}}}
\bibcite{idris2014python}{\citeauthoryear {Idris}{Idris}{{\APACyear {2014}}}}
\APACbibcite{idris2014python}{\citeauthoryear {Idris}{Idris}{{\APACyear {2014}}}}
\bibcite{james2013introduction}{\citeauthoryear {James, Witten, Hastie,{}\ \BBA{} Tibshirani}{James\ \BOthers {.}}{{\APACyear {2013}}}}
\APACbibcite{james2013introduction}{\citeauthoryear {James, Witten, Hastie,{}\ \BBA{} Tibshirani}{James\ \BOthers {.}}{{\APACyear {2013}}}}
\bibcite{kounios2009aha}{\citeauthoryear {Kounios\ \BBA{} Beeman}{Kounios\ \BBA{} Beeman}{{\APACyear {2009}}}}
\APACbibcite{kounios2009aha}{\citeauthoryear {Kounios\ \BBA{} Beeman}{Kounios\ \BBA{} Beeman}{{\APACyear {2009}}}}
\bibcite{lacroix2006introduction}{\citeauthoryear {Lacroix\ \BBA{} Cousineau}{Lacroix\ \BBA{} Cousineau}{{\APACyear {2006}}}}
\APACbibcite{lacroix2006introduction}{\citeauthoryear {Lacroix\ \BBA{} Cousineau}{Lacroix\ \BBA{} Cousineau}{{\APACyear {2006}}}}
\@writefile{toc}{\contentsline {chapter}{References}{51}}
\bibcite{murre2011power}{\citeauthoryear {Murre\ \BBA{} Chessa}{Murre\ \BBA{} Chessa}{{\APACyear {2011}}}}
\APACbibcite{murre2011power}{\citeauthoryear {Murre\ \BBA{} Chessa}{Murre\ \BBA{} Chessa}{{\APACyear {2011}}}}
\bibcite{newell1981mechanisms}{\citeauthoryear {Newell\ \BBA{} Rosenbloom}{Newell\ \BBA{} Rosenbloom}{{\APACyear {1981}}}}
\APACbibcite{newell1981mechanisms}{\citeauthoryear {Newell\ \BBA{} Rosenbloom}{Newell\ \BBA{} Rosenbloom}{{\APACyear {1981}}}}
\bibcite{python2016pickle}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {1}}}}}}
\APACbibcite{python2016pickle}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {1}}}}}}
\bibcite{pythonLeastSquare}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {2}}}}}}
\APACbibcite{pythonLeastSquare}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {2}}}}}}
\bibcite{rogers2015first}{\citeauthoryear {Rogers\ \BBA{} Girolami}{Rogers\ \BBA{} Girolami}{{\APACyear {2015}}}}
\APACbibcite{rogers2015first}{\citeauthoryear {Rogers\ \BBA{} Girolami}{Rogers\ \BBA{} Girolami}{{\APACyear {2015}}}}
\bibcite{shafranovich2008common}{\citeauthoryear {Shafranovich}{Shafranovich}{{\APACyear {2008}}}}
\APACbibcite{shafranovich2008common}{\citeauthoryear {Shafranovich}{Shafranovich}{{\APACyear {2008}}}}
\bibcite{stafford2014tracing}{\citeauthoryear {Stafford\ \BBA{} Dewar}{Stafford\ \BBA{} Dewar}{{\APACyear {2014}}}}
\APACbibcite{stafford2014tracing}{\citeauthoryear {Stafford\ \BBA{} Dewar}{Stafford\ \BBA{} Dewar}{{\APACyear {2014}}}}
\bibcite{stafford2016testing}{\citeauthoryear {Stafford\ \BBA{} Haasnoot}{Stafford\ \BBA{} Haasnoot}{{\APACyear {2016}}}}
\APACbibcite{stafford2016testing}{\citeauthoryear {Stafford\ \BBA{} Haasnoot}{Stafford\ \BBA{} Haasnoot}{{\APACyear {2016}}}}
\bibcite{stonebraker2013data}{\citeauthoryear {Stonebraker\ \BOthers {.}}{Stonebraker\ \BOthers {.}}{{\APACyear {2013}}}}
\APACbibcite{stonebraker2013data}{\citeauthoryear {Stonebraker\ \BOthers {.}}{Stonebraker\ \BOthers {.}}{{\APACyear {2013}}}}
\bibcite{stuart2009Axon}{\citeauthoryear {Stuart}{Stuart}{{\APACyear {2009}}}}
\APACbibcite{stuart2009Axon}{\citeauthoryear {Stuart}{Stuart}{{\APACyear {2009}}}}
\bibcite{sutton1998reinforcement}{\citeauthoryear {Sutton\ \BBA{} Barto}{Sutton\ \BBA{} Barto}{{\APACyear {1998}}}}
\APACbibcite{sutton1998reinforcement}{\citeauthoryear {Sutton\ \BBA{} Barto}{Sutton\ \BBA{} Barto}{{\APACyear {1998}}}}
