\relax 
\citation{newell1981mechanisms}
\citation{lacroix2006introduction}
\citation{newell1981mechanisms}
\citation{heathcote2000power}
\citation{gallistel2004learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background and Motivation}{1}}
\citation{stafford2014tracing}
\citation{stafford2014tracing}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Definition}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Aims and Objectives}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Project Management}{3}}
\citation{stuart2009Axon}
\citation{stonebraker2013data}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Data Curation}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{stafford2014tracing}
\citation{stafford2016testing}
\citation{stafford2014tracing}
\citation{stafford2016testing}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data Cleaning}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Data accommodating}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Original variables in the data source}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Hidden variables calculated from the original variables}{6}}
\citation{stafford2016testing}
\citation{shafranovich2008common}
\citation{idris2014python}
\citation{idris2014python}
\citation{python2016pickle}
\citation{python2016pickle}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Data Deformation}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Pickle file format}{8}}
\citation{newell1981mechanisms}
\citation{gallistel2004learning}
\citation{donner2015piecewise}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Data storage and index table}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Multiprocessing}{9}}
\citation{donner2015piecewise}
\citation{hackeling2014mastering}
\citation{harrington2012machine}
\citation{daume2012course}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}K-Means Clustering with Competitive Learning Algorithm}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}K-Means clustering}{11}}
\newlabel{eq:threshold1}{{2.1}{11}}
\newlabel{eq:k_means_cost_function}{{2.2}{11}}
\newlabel{eq:threshold2}{{2.3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Competitive learning}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Competitive neuron diagram. When an input enters the network, it generates output values according to weights between an input and output neuron (blue arrows). Output neurons compete with each other (red arrows), then only one output fires on an input pattern. Output neurons are considered only having binary $0/1$ output.\relax }}{13}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Competitive_Diagram}{{2.1}{13}}
\citation{bowles2015machine}
\newlabel{eq:competitive1}{{2.4}{14}}
\newlabel{eq:competitive2}{{2.5}{14}}
\newlabel{eq:competitive3}{{2.6}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Normalisation}{14}}
\newlabel{eq:inner_product}{{2.7}{14}}
\newlabel{fig:not_norm_data}{{2.2a}{15}}
\newlabel{sub@fig:not_norm_data}{{a}{15}}
\newlabel{fig:not_norm_weight}{{2.2b}{15}}
\newlabel{sub@fig:not_norm_weight}{{b}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Examples of not-normalised data and weight in 3 dimensions. In both left and right figures, sphere represents unit length area.\relax }}{15}}
\newlabel{fig:not-normalised}{{2.2}{15}}
\newlabel{eq:distance}{{2.8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Procedure of the algorithm}{16}}
\citation{hackeling2014mastering}
\newlabel{eq:on-line and batch}{{2.9}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Local optima and optimal number of clusters}{17}}
\newlabel{sbs:number_clusters}{{2.4.5}{17}}
\citation{daume2012course}
\newlabel{eq:AICandBIC}{{2.10}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}Dealing with dead unit}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison between AIC and BIC according to the different number of k. AIC are more strict method than BIC to identify the number of clusters.\relax }}{19}}
\newlabel{fig:AIC_BIC_Elbow}{{2.3}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces An example of dead unit problem. Red and Green arrows are initialised weight before applying into the network. Green arrow is set far from both two input clusters, and it is assumed as a dead unit. Unfortunately, green arrow will never fire through the learning, and which means that input vectors are not properly clustered.\relax }}{20}}
\newlabel{fig:dead_unit}{{2.4}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Learning rate and existence of dead units\relax }}{21}}
\newlabel{fig:dead_state}{{2.5}{21}}
\newlabel{eq:leaky_learning}{{2.11}{21}}
\newlabel{eq:add_bias}{{2.12}{22}}
\newlabel{eq:add_noise}{{2.13}{22}}
\newlabel{eq:SV}{{2.14}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Learning rate and and entire firing number of output neurons after using SV approach for dead units\relax }}{23}}
\newlabel{fig:dead_states_solved}{{2.6}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.7}Centroids}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Centroids of cluster from 1 to 4\relax }}{24}}
\newlabel{fig:centroid1}{{2.7}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Centroids of cluster from 5 to 8\relax }}{25}}
\newlabel{fig:centroid2}{{2.8}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Centroids of cluster from 9 to 12\relax }}{25}}
\newlabel{fig:centroid3}{{2.9}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Centroids of cluster 13 and 14\relax }}{26}}
\newlabel{fig:centroid4}{{2.10}{26}}
\citation{james2013introduction}
\citation{rogers2015first}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Function Fitting}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:unbiased}{{3.1}{27}}
\citation{howard2014learning}
\citation{murre2011power}
\citation{gallistel2004learning}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Functions}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Parametric Method}{28}}
\newlabel{sbs:Parametric Method}{{3.1.1}{28}}
\newlabel{eq:first_order}{{3.2}{29}}
\newlabel{eq:vector_matrix}{{3.3}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Functions for Single Curve}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Possible 4 models; first order polynomial, 3 parameters exponential function, 4 parameters power function, step-like function with change point at 5\relax }}{30}}
\newlabel{fig:Functions}{{3.1}{30}}
\citation{heathcote2000power}
\newlabel{eq:high_order}{{3.4}{31}}
\newlabel{eq:exponential_function}{{3.5}{31}}
\newlabel{eq:power_function}{{3.6}{32}}
\newlabel{eq:step_like}{{3.7}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Piecewise Function}{32}}
\newlabel{sbs:piecewise}{{3.1.3}{32}}
\citation{gallistel2004learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Example of piecewise functions; exponential and power functions\relax }}{33}}
\newlabel{fig:Piecewise_Functions}{{3.2}{33}}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Method}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Squared Lost Function}{34}}
\newlabel{sbs:square_lost_function}{{3.2.1}{34}}
\newlabel{eq:loss_function}{{3.10}{35}}
\newlabel{eq:avg_loss_function}{{3.11}{35}}
\newlabel{eq:loss_derivative}{{3.12}{35}}
\newlabel{eq:loss_zero}{{3.13}{35}}
\citation{pythonLeastSquare}
\citation{rogers2015first}
\citation{rogers2015first}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Training Set and Validation Set}{37}}
\newlabel{sbs:Training_Validation}{{3.2.2}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Over-fitting and Regularisation}{37}}
\newlabel{sbs:over-fitting}{{3.2.3}{37}}
\newlabel{fig:not_norm_data}{{3.3a}{38}}
\newlabel{sub@fig:not_norm_data}{{a}{38}}
\newlabel{fig:not_norm_weight}{{3.3b}{38}}
\newlabel{sub@fig:not_norm_weight}{{b}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Minimum cost in training set, but large cost in validation set. Exponential function fitting with 3 parameters to 15 training set of cluster 13, and validation check\relax }}{38}}
\newlabel{fig:not-normalised}{{3.3}{38}}
\newlabel{fig:not_norm_data}{{3.4a}{38}}
\newlabel{sub@fig:not_norm_data}{{a}{38}}
\newlabel{fig:not_norm_weight}{{3.4b}{38}}
\newlabel{sub@fig:not_norm_weight}{{b}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The learning curve which has the minimum cost in validation set. Exponential function fitting with 3 parameters to 15 training set of cluster 1, and validation check\relax }}{38}}
\newlabel{fig:not-normalised}{{3.4}{38}}
\citation{rogers2015first}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Choice of Breaking Point}{39}}
\newlabel{abs:breaking_point}{{3.2.4}{39}}
\newlabel{eq:cost_matrix}{{3.15}{39}}
\newlabel{eq:length_matrix}{{3.16}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Transition point shift in 4 pieces\relax }}{40}}
\newlabel{fig:pieces}{{3.5}{40}}
\bibstyle{apacite}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Curve Fittings}{41}}
\bibcite{bowles2015machine}{\citeauthoryear {Bowles}{Bowles}{{\APACyear {2015}}}}
\APACbibcite{bowles2015machine}{\citeauthoryear {Bowles}{Bowles}{{\APACyear {2015}}}}
\bibcite{daume2012course}{\citeauthoryear {Daum{\'e}\nobreakspace  {}III}{Daum{\'e}\nobreakspace  {}III}{{\APACyear {2012}}}}
\APACbibcite{daume2012course}{\citeauthoryear {Daum{\'e}\nobreakspace  {}III}{Daum{\'e}\nobreakspace  {}III}{{\APACyear {2012}}}}
\bibcite{donner2015piecewise}{\citeauthoryear {Donner\ \BBA{} Hardy}{Donner\ \BBA{} Hardy}{{\APACyear {2015}}}}
\APACbibcite{donner2015piecewise}{\citeauthoryear {Donner\ \BBA{} Hardy}{Donner\ \BBA{} Hardy}{{\APACyear {2015}}}}
\bibcite{gallistel2004learning}{\citeauthoryear {Gallistel, Fairhurst,{}\ \BBA{} Balsam}{Gallistel\ \BOthers {.}}{{\APACyear {2004}}}}
\APACbibcite{gallistel2004learning}{\citeauthoryear {Gallistel, Fairhurst,{}\ \BBA{} Balsam}{Gallistel\ \BOthers {.}}{{\APACyear {2004}}}}
\bibcite{hackeling2014mastering}{\citeauthoryear {Hackeling}{Hackeling}{{\APACyear {2014}}}}
\APACbibcite{hackeling2014mastering}{\citeauthoryear {Hackeling}{Hackeling}{{\APACyear {2014}}}}
\bibcite{harrington2012machine}{\citeauthoryear {Harrington}{Harrington}{{\APACyear {2012}}}}
\APACbibcite{harrington2012machine}{\citeauthoryear {Harrington}{Harrington}{{\APACyear {2012}}}}
\bibcite{heathcote2000power}{\citeauthoryear {Heathcote, Brown,{}\ \BBA{} Mewhort}{Heathcote\ \BOthers {.}}{{\APACyear {2000}}}}
\APACbibcite{heathcote2000power}{\citeauthoryear {Heathcote, Brown,{}\ \BBA{} Mewhort}{Heathcote\ \BOthers {.}}{{\APACyear {2000}}}}
\bibcite{howard2014learning}{\citeauthoryear {Howard}{Howard}{{\APACyear {2014}}}}
\APACbibcite{howard2014learning}{\citeauthoryear {Howard}{Howard}{{\APACyear {2014}}}}
\bibcite{idris2014python}{\citeauthoryear {Idris}{Idris}{{\APACyear {2014}}}}
\APACbibcite{idris2014python}{\citeauthoryear {Idris}{Idris}{{\APACyear {2014}}}}
\bibcite{james2013introduction}{\citeauthoryear {James, Witten, Hastie,{}\ \BBA{} Tibshirani}{James\ \BOthers {.}}{{\APACyear {2013}}}}
\APACbibcite{james2013introduction}{\citeauthoryear {James, Witten, Hastie,{}\ \BBA{} Tibshirani}{James\ \BOthers {.}}{{\APACyear {2013}}}}
\bibcite{murre2011power}{\citeauthoryear {Murre\ \BBA{} Chessa}{Murre\ \BBA{} Chessa}{{\APACyear {2011}}}}
\APACbibcite{murre2011power}{\citeauthoryear {Murre\ \BBA{} Chessa}{Murre\ \BBA{} Chessa}{{\APACyear {2011}}}}
\bibcite{newell1981mechanisms}{\citeauthoryear {Newell\ \BBA{} Rosenbloom}{Newell\ \BBA{} Rosenbloom}{{\APACyear {1981}}}}
\APACbibcite{newell1981mechanisms}{\citeauthoryear {Newell\ \BBA{} Rosenbloom}{Newell\ \BBA{} Rosenbloom}{{\APACyear {1981}}}}
\bibcite{python2016pickle}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {1}}}}}}
\APACbibcite{python2016pickle}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {1}}}}}}
\@writefile{toc}{\contentsline {chapter}{References}{42}}
\bibcite{pythonLeastSquare}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {2}}}}}}
\APACbibcite{pythonLeastSquare}{\citeauthoryear {python on-line document}{python on-line document}{{\APACyear {{\bibnodate {}}}}{\APACexlab {{\BCntND {2}}}}}}
\bibcite{rogers2015first}{\citeauthoryear {Rogers\ \BBA{} Girolami}{Rogers\ \BBA{} Girolami}{{\APACyear {2015}}}}
\APACbibcite{rogers2015first}{\citeauthoryear {Rogers\ \BBA{} Girolami}{Rogers\ \BBA{} Girolami}{{\APACyear {2015}}}}
\bibcite{shafranovich2008common}{\citeauthoryear {Shafranovich}{Shafranovich}{{\APACyear {2008}}}}
\APACbibcite{shafranovich2008common}{\citeauthoryear {Shafranovich}{Shafranovich}{{\APACyear {2008}}}}
\bibcite{stafford2014tracing}{\citeauthoryear {Stafford\ \BBA{} Dewar}{Stafford\ \BBA{} Dewar}{{\APACyear {2014}}}}
\APACbibcite{stafford2014tracing}{\citeauthoryear {Stafford\ \BBA{} Dewar}{Stafford\ \BBA{} Dewar}{{\APACyear {2014}}}}
\bibcite{stafford2016testing}{\citeauthoryear {Stafford\ \BBA{} Haasnoot}{Stafford\ \BBA{} Haasnoot}{{\APACyear {2016}}}}
\APACbibcite{stafford2016testing}{\citeauthoryear {Stafford\ \BBA{} Haasnoot}{Stafford\ \BBA{} Haasnoot}{{\APACyear {2016}}}}
\bibcite{stonebraker2013data}{\citeauthoryear {Stonebraker\ \BOthers {.}}{Stonebraker\ \BOthers {.}}{{\APACyear {2013}}}}
\APACbibcite{stonebraker2013data}{\citeauthoryear {Stonebraker\ \BOthers {.}}{Stonebraker\ \BOthers {.}}{{\APACyear {2013}}}}
\bibcite{stuart2009Axon}{\citeauthoryear {Stuart}{Stuart}{{\APACyear {2009}}}}
\APACbibcite{stuart2009Axon}{\citeauthoryear {Stuart}{Stuart}{{\APACyear {2009}}}}
