% ----------------------------------------------------------------
% Report Class (This is a LaTeX2e document)  *********************
% ----------------------------------------------------------------
% ACS 322/420 Dissertation Template
% March 2013, JL

\documentclass[12pt,a4paper]{report}
\usepackage[lmargin=3.81cm,tmargin=2.54cm,rmargin=2.54cm,bmargin=2.52cm]{geometry}
\linespread{1.5}
% Fonts -- the following two packages give your Times Roman in LaTex
%\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage{mathtools} %argmin
\usepackage{verbatim} 
\usepackage{algorithmic}
\usepackage{relsize}
\usepackage{graphicx}
%\usepackage[nottoc]{tocbibind} 
\usepackage{apacite}
%\graphicspath{ {/Users/PureMac/Desktop/Learning_Analytics/Figs/} }
\graphicspath{ {Figs/} }

\begin{document}

\pagenumbering{Roman}

\newpage \section*{ABSTRACT}

% This section is compulsory -- guidelines can be found in the presentation about writing the dissertation.

\newpage \section*{ACKNOWLEDGEMENTS}


%%% Table of Contents

\tableofcontents

\thispagestyle{empty}


% ======================================================== %
%		1. Introduction
% ======================================================== %
\chapter{Introduction}

\pagenumbering{arabic}
\setcounter{page}{1}

% ======================================= %
%		1.1 Background and Motivation
% ======================================= %
\section{Background and Motivation}

% Conditioning paradigms reflects the "neurobiological basis of learning and memory" (Gallistel 2004).  
Learning curves may reflect different underlying learning processes. For example, in insight learning, it may follow slightly fluctuated level and sudden dramatic increase at some points. Because the learning is sometimes the result of experience through personal interactions with the environment. And following the occurrence of insight, abrupt realisation of how to solve the problem can be repeated in future similar situations. This means that the associated experiences with insights can be parameterised, and it can be linked to future behaviours.
% Also, experiences and insight may reinforce each other and can be associated to one another. This associated experience can be memorised and be a feedback as a parameter for future learning. 
% Eventually, behaviour can memorise stimuli and feedbacks, and these may affect and eventually change behaviour.
% insight learning, associative learning, multi-component/process learning.

% These learning processes are reinforced by the conditioned stimulus
% Insight learning is the first cognitive learning.  
% https://psychlopedia.wikispaces.com/insight+learning

%Associative learning : a learning principle that states ideas and experiences reinforce each other and can be mentally linked to one another. Conditioning; behaviour can be modified or learned based on a stimulus and a response. Reinforcers; stimuli used to change behaviour.

Therefore, there are several possible functions which might fit learning curves and represent different learning processes; step function, powerlaw, piecewise powerlaw and etc. Through possible functions and learning curves, people can be grouped into several learning patterns. Moreover, by inspecting people in those groups, 'how they practice' to get the learning curve and 'which features' having an affect upon individuals can be found. Then suggestions for better learning for individuals may be categorised and proposed.

% By fitting those functions to the big data of individuals' game result with various features,
% may influence the individuals' learning curve' 
% on their learning process'

For this, online games are a reasonable instrument. As it involves "rapid perception, decision making, and motor responding", as well as it gives rich details of practice history, in order to find individuals' learning curve and investigate their features (Stafford and Dewar, 2013) 

%\textit{An online game involving rapid perception, decision making, and motor responding. Use of game data-> rich details of training history with measures of performance. Relations between \textbf{practice amount} and \textbf{subsequent performance}, and between \textbf{practice spacing} and \textbf{subsequent performance.}\\
%Greater initial variation in performance is linked to higher subsequent performance-> \textbf{a result we link to the exploration/exploitation trade-off from the computational framework of reinforcement learning.}//
%Explaining two dilemmas: \\
%Reinforcement learning, exploration/exploitation dilemma}

% \subsection{[level 3 heading]}

% As a general rule don't use more than three levels for a heading

Text...

% ======================================= %
%		1.2. Problem Definition
% ======================================= %

\section{Problem Definition}

There are three main issues in traditional reinforcement learning curve. Because it has smooth power law with diminishing gain derived from average value, all possible individual learning curve are squashed into one learning curve. Thus, it cannot tell which learning curve will belong to a task, behaviour, and learning process. Furthermore, even though identifying bad or good on learnings are important, but it cannot provide sufficient information about those.

Below is the list of problems on current reinforcement learning curve.

\begin{itemize}
\item Not being able to categorise learning process.
\item Averaging possible individual learning curves
\item Not enough information for identification of learning success
\end{itemize}
%Explaining about the problem of bootstrapping analysis.\\
%https://en.wikipedia.org/wiki/Bootstrapping (statistics)\\
% "Unless one is reasonably sure that the underlying distribution is not heavy tailed, one should hesitate to use the naive bootstrap"\\
%Bootstrap of the mean in the infinite variance case Athreya, K.B. Ann Stats vol 15 (2) 1987 724-731\\

%%%%%%%%%%%%%%%%%%%
% Piecewise powerlaw? ????? ??? ???? ??? curve? ?? ???? 
% ??? ? ????!
%%%%%%%%%%%%%%%%%%%

% This is a suggested section heading

% ======================================= %
%		1.3. Aims and Objectives
% ======================================= %

\section{Aims and Objectives}
%%%%%
These are the concepts I think you should cover in your introduction:

Skill acquisition \& expertise
- factors which are known to influence skill acquisition
- practice amount, practice spacing, others

Using games to study skill acquisition

Learning curves
- different possible underlying functions
- problems with averaging over

The ultimate aim, of course, is to identify individuals who learn faster or for longer, and try and relate this to how they practice
%%%%%
The aim of this paper is to fit different functions to the Axon game data. In the data, there are 854,064 individuals data about when, where they did the game and other informations, as well as scores of game according to their attempts. And so identify the underlying learning process. Furthermore, it is to inspect features which may have influences on the learning curves. 

\begin{itemize}
\item Understand several possible functions; step function, powerlaw, piecewise powerlaw and etc.
\item Understand different underlying learning processes; insight learning, associative learning, multi-component/process learning and etc.
\item Fit functions to individual Axon game data and compare those to identify.
\item Test theories of what makes learning most effective, exploiting unsupervised learning, establish which parameter is important for getting learning curve from the data.
\item Design more effective learning practices.
\end{itemize}
% This section is compulsory -- guidelines can be found in the presentation about writing the dissertation

%The aim(s) should provide a general idea of what the project is about and the objectives should be more specific. The objectives should be actions that you expect to do (e.g. simulate..., test..., compare...). At the end of the project it should be possible to assess which of the objectives have been achieved. The objectives will also link into the work programme (below).

% ======================================= %
%		1.4. Project Management
% ======================================= %

\section{Project Management}

% This section is compulsory -- guidelines can be found in the presentation about writing the dissertation

Text...

% ======================================================== %
%		2. Data Curation
% ======================================================== %


\chapter{Data Curation}
A large number of data were acquired through the online game named Axon which is developed by Preload for Welcome trust \cite{stuart2009Axon}. Totally 1,201,516 machine identities (or players) played the game over 4 million times. The raw data set is fundamentally comprised by the score, date, time of plays and so on in accordance with machine identities which may represent each individual. %\cite{stafford2014tracing}. 
Undoubtedly, it can be possible to extract information from the data set on how people practiced to get a higher score. For example, it shows how much time they played for each score. 
However, the raw data set of on-line game seems quite noisy to discern those factors which affect player's learning. It contained many 'undefined' or 'unrepresentable values', and some values are not valid, such as starting play time is later than the time play finished. For this reason, the information on what the analysis needs cannot be identified directly from it.

Thus, the data source needs refining processes until the gemstone of data set could gleam with the evidence of valuable information. In other word, the big data set has to be curated. Stonebraker et al. stated that "data curation is the act of discovering a data source(s) of interest, cleaning and transforming the new data, semantically integrating it with other local data sources, and deduplicating the resulting composite" \cite{stonebraker2013data}.

\bigskip
Below are the tasks in which steps of data curation are largely expected,

% that this dissertation is interested in, and especially where
\begin{enumerate}
	\item Cleaning raw data set,
	%\item Grouping filtered data set according to the same machine ID,
	\item Incrementally accommodating new data entities,
	\item Normalising scores grouped by the same players,
	\item Clustering players, showing the similar patterns.
\end{enumerate}

Especially, data curation tasks followed methods of two previous studies
\cite{stafford2014tracing, stafford2016testing}.
Accordingly, the curation tasks of cleaning, grouping by machine IDs and accommodating will be explained at the following section. After then, another tasks for data deformation and clustering will be examined.

\begin{comment}
which certified "the relationship between the practice amount and subsequent performance, and between practice spacing and subsequent performance" and "the consolidation effect for skill learning". In order for these analyses, unnecessary information contained in the data set had to be filtered, and additionally, the new variables were augmented by manifesting some hidden features in the same player through those two analyses.

In gratitude, filtering methods on the data source was already regularized in some degree by Stafford et al. (2014; 2016).
\end{comment}
% , K-Means clustering, with the artificial neural network algorithm, competitive learning 

% ======================================= %
%		2.1. Data Cleaning
% ======================================= %
\section{Data Cleaning}

What has to be noted before data cleaning is that it has to be implemented under the grouped data by the same machine IDs. Totally 4,038,802 plays were sorted by players having the same machine IDs, and then 1,201,516 players were detected. The criteria according to the previous two studies \cite{stafford2014tracing, stafford2016testing} are as follow,
% Though the two studies in 2014 and 2016 about skill learning analysis using the game data source, some curation steps were performed by the following criteria.

%\begin{itemize}
%	\item 26,727 players who did not play a minimum of 15 games were eliminated,
%	\item 8\% out of the total number of players attempted more than 300 times, and those were also discharged.
%	\item 26,175 data which lack valid longitude or timing information for each attempt were filtered.
%\end{itemize}

\begin{enumerate}
	\item players who did not play at least 15 games,
	\item players who attempted more than 300 times,
	\item data which lack valid longitude or timing information for each attempt,
	\item players with discontinuous game play attempts.
\end{enumerate}

The reason of discarding those who played less than 15 attempts is because those cannot show a causal relationship between attempts and scores. In this step, xxx individuals were deleted. 
And the reason of second step is that plays more than 300 attempts did not show many differences against after 300 attempts. ??? out of total number of players attempted the game more than 300 times, and also they were removed. Also discarded players having unrecognisable time information were ???. Finally, game play data for each individual recorded discontinuously. In other word, there are non-recorded game play data between the first attempt and the last attempt for each players. The reason why this problem happened is unknown though, it is obvious that individuals with this issue have to be filtered appropriately. Though cleaning those is simply mentioned in this session, the specific method how to manage players with discontinuous data will be discussed in chapter ???.

As a result, raw data with 4, 038,802 plays with xxx players were cleaned, and then reduced to ??? plays and ??? individuals decreased by ???.

\section{Data accommodating}
The next step of data curation is to incrementally accommodate hidden but obvious features from the previous cleaned data set. It is originally constituted by basic 8 categories. After data accommodating task, 7 additional hidden variable would be derived.

\subsection{Original variables in the data source}
When people play the on-line game, 7 basic information as follow is recorded,
\begin{itemize}
    \item machine identities,
	\item scores and attempt numbers,
	\item date, hour and minute,
	\item latitude and longitude.
\end{itemize}

When players access the on-line game, tracking code written by Preloaded record machine identities. Machine IDs may be considered as individuals who actually play the game. Preloaded inserted tracking code to the on-line game, which records a machine identity at each time the game was loaded, as well as the result of the game at the end of plays, and play date and time. Attempts and scores were recorded in the order of the time.

The information of locations where the machine ID accessed the game is also collected approximately at the maximum of city-block level.

\subsection{Hidden variables calculated from the original variables}
From the basic variables which are recorded when people play the game, other features that may imply useful information of how players are influenced during learning processes to get higher marks can be derived as bellow,

\begin{itemize}
	\item time difference,
	\item local time,
	\item gap types (no, long, short and sleep gap),
	\item a number of total plays.
\end{itemize}

The basic time information; date, hour and minute is combined together. And then, time differences between successive attempts are calculated by subtracting current game start time from its previous attempt time. Therefore, the first attempts do not have value of time differences.

\bigskip
Next, local times for each play were calculated in seconds, using the formula, "$\textit{local time} = \textit{UTCtime} + (\textit{longitude} × 24\div360),\textit{modulo}24$" \cite{stafford2016testing}. Local time would facilitate comparison of players in different time zone at the same level as life style time. There was data that local time could not be calculated because of missing tack of latitude and longitude, and those were filtered at the data cleaning task.

\bigskip
On the basis of time difference, Stafford et al. (2016) classified individuals into 4 groups, according to "the nature of the timing of their first 15 attempts at the game". First group is 'no gap' for those who had less than 15 minute break between each play. Second group is 'wake' in which players are assumed having a rest in there waking hours if they have "a single gap between 7 and 12 hours". Working hours is time between 5 am and 12 pm. And those who rested for between 7 and 12 hours in working hours were categorised as 'sleep'. Finally, all other individuals are classified as "no category".

Furthermore, for more detailed analysis, 'rest' which is defined in 'Gap types' categorisation was divided with 3 types. Long gap, short gap and sleep gap were determined when players had a rest 'between 7 hours and 11 hours', 'below 15 minutes', and 'not in working hours' respectively.

\bigskip
As the term of hidden variables, there are possibilities that more factors can be mined from the data set and be incrementally accommodated to it if appropriate methods are implemented.

\section{Data Deformation}
CSV format has been used as a basic data file format for previous study.

CSV is an abbreviation of Comma-Separated Values. A data record of CSV format file is a line formed by one or more elements, and commas separate those element values to be distinguishable (plain text format) \cite{shafranovich2008common}. According to Idris (2014), it is straightforward for those format to load and store data-set from the data storage onto working place; ease to generate, read and edit manually, versatility in most programming languages, but not efficient because exploiting plain text format "take a lot of space". And this might result in consuming more time to handle such a big data set than using binary format.

Simply, the data set needs to be stored in memory unit of machine(s) where the analysis processes would operate. However, these are true only when data size is manageable. Because fitting all data-set of such a big data in computer memory takes many spaces. In some case loading on computer memory cannot be possible depending on computer capability and setup. Thus, CSV format is not very efficient way to manage such a big data \cite{idris2014python}. To solve with those problems, two approaches of being able to reduce storing spaces and of enabling faster speed for analysing were applied; converting CSV file format into Pickle format, and splitting data set into several pieces.

\subsection{Pickle file format}
Pickle file format can provide "a high level of data compression such as zip, bzip and gzip" and also faster than CSV \cite{idris2014python}, because it is "binary protocols for serializing and de-serializing a Python object structure" \cite{python2016pickle}. Furthermore, it provides easy approach to deal with converting Python object hierarchy into a binary stream and vice versa, with the notion of 'pickling' and 'unpickling'. Moreover, pickle file format can automatically represent a large number of Python types \cite{python2016pickle}.

\bigskip
Therefore, curated data set after cleaning, accommodating, and deformation process is stored as binary file format, and for the binary file format, pickle specified for Python is used.

\subsection{Data index table and separation}
Although the big data set can be converted to binary file, it still takes large amount of storage. And it also causes RAM memory optimisation problem. When data curation tasks performed (of course, as well as data analysis task which will be discussed in the next session), processing speed became slower and slower over time. This might be because data occupies all spaces of hardware memory, and exploits virtual memory on main memory unit. Basically, RAM memory provides very high speed compared to hard disk, and it is even faster than Solid State Drive (SSD) which is often considered 'fast'. If data takes all RAM memory spaces, it transfer addresses of some part of it to virtual memory. This task causes many interruptions to the tasks, and eventually decrease processing performance remarkably. 

\bigskip
Therefore, loading only necessary data to a specific task will be a solution to keep processing performance fast. Whole data set was separated by each type of variables. To secure not to lose player information within the divided data, address of each data has to be indexed directing player ID. As a result of data deformation task, 15 pickle data files were generated; eventLabel, date, hour, minute, latitude, longitude, eventValue, play\_filter, comb\_time, diff\_time, local\_time, long\_gap, shor\_gap, sleep\_gap, gap\_type, and total\_plays, which are pointed by individual ID information, eventAction.

%%%%%% comment %%%%%%%%%%%%%
\begin{comment}
(Python 2001)
There are various file formats which can provide a high level of data compression.

, which is formed from 15 factors, into the data vector having like hash table

By default, the pickle data format uses a relatively compact binary representation. If you need optimal size characteristics, you can efficiently compress pickled data.


?CSV is human readable and easy to edit manually
?CSV is simple to implement and parse
?CSV is processed by almost all existing applications
?CSV provides a straightforward information schema 
?CSV is faster to handle
?CSV is smaller in size
?CSV is considered to be standard format
?CSV is compact. For XML you start tag and end tag for each column in each row. In CSV you write the column headers only once.
?CSV is easy to generate

Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate data-set, store them on disk and curate them independently. Later we'll merge them into a single data-set of manageable size.
\end{comment}

% ======================================= %
%		2.4. K-Means Clustering with Competitive Learning Algorithm
% ======================================= %
\section{K-Means Clustering with Competitive Learning Algorithm}

%\nocite{*}
\bibliographystyle{apacite}
%\bibliographystyle{apalike}
\bibliography{references}
\end{document}