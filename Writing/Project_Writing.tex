% ----------------------------------------------------------------
% Report Class (This is a LaTeX2e document)  *********************
% ----------------------------------------------------------------
% ACS 322/420 Dissertation Template
% March 2013, JL

\documentclass[12pt,a4paper]{report}
\usepackage[lmargin=3.81cm,tmargin=2.54cm,rmargin=2.54cm,bmargin=2.52cm]{geometry}
\linespread{1.5}
% Fonts -- the following two packages give your Times Roman in LaTex
%\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage{mathtools} %argmin
\usepackage{verbatim} 
\usepackage{algorithmic}
\usepackage{relsize}

\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage[nottoc]{tocbibind} 
\usepackage{apacite}
%\graphicspath{ {/Users/PureMac/Desktop/Learning_Analytics/Figs/} }
\graphicspath{ {Figs/} }

\begin{document}

\pagenumbering{Roman}

\newpage \section*{ABSTRACT}

% This section is compulsory -- guidelines can be found in the presentation about writing the dissertation.

\newpage \section*{ACKNOWLEDGEMENTS}


%%% Table of Contents

\tableofcontents

\thispagestyle{empty}


% ======================================================== %
%		1. Introduction
% ======================================================== %
\chapter{Introduction}

\pagenumbering{arabic}
\setcounter{page}{1}

% ======================================= %
%		1.1 Background and Motivation
% ======================================= %
\section{Background and Motivation}

% Conditioning paradigms reflects the "neurobiological basis of learning and memory" (Gallistel 2004).  
Learning curves may reflect different underlying learning processes. For example, in insight learning, it may follow slightly fluctuated level and sudden dramatic increase at some points. Because the learning is sometimes the result of experience through personal interactions with the environment. And following the occurrence of insight, abrupt realisation of how to solve the problem can be repeated in future similar situations. This means that the associated experiences with insights can be parameterised, and it can be linked to future behaviours.
% Also, experiences and insight may reinforce each other and can be associated to one another. This associated experience can be memorised and be a feedback as a parameter for future learning. 
% Eventually, behaviour can memorise stimuli and feedbacks, and these may affect and eventually change behaviour.
% insight learning, associative learning, multi-component/process learning.

% These learning processes are reinforced by the conditioned stimulus
% Insight learning is the first cognitive learning.  
% https://psychlopedia.wikispaces.com/insight+learning

%Associative learning : a learning principle that states ideas and experiences reinforce each other and can be mentally linked to one another. Conditioning; behaviour can be modified or learned based on a stimulus and a response. Reinforcers; stimuli used to change behaviour.

Therefore, there are several possible functions which might fit learning curves and represent different learning processes; step function, powerlaw, piecewise powerlaw and etc. Through possible functions and learning curves, people can be grouped into several learning patterns. Moreover, by inspecting people in those groups, 'how they practice' to get the learning curve and 'which features' having an affect upon individuals can be found. Then suggestions for better learning for individuals may be categorised and proposed.

% By fitting those functions to the big data of individuals' game result with various features,
% may influence the individuals' learning curve' 
% on their learning process'

For this, online games are a reasonable instrument. As it involves "rapid perception, decision making, and motor responding", as well as it gives rich details of practice history, in order to find individuals' learning curve and investigate their features (Stafford and Dewar, 2013) 

%\textit{An online game involving rapid perception, decision making, and motor responding. Use of game data-> rich details of training history with measures of performance. Relations between \textbf{practice amount} and \textbf{subsequent performance}, and between \textbf{practice spacing} and \textbf{subsequent performance.}\\
%Greater initial variation in performance is linked to higher subsequent performance-> \textbf{a result we link to the exploration/exploitation trade-off from the computational framework of reinforcement learning.}//
%Explaining two dilemmas: \\
%Reinforcement learning, exploration/exploitation dilemma}

% \subsection{[level 3 heading]}

% As a general rule don't use more than three levels for a heading

Text...

% ======================================= %
%		1.2. Problem Definition
% ======================================= %

\section{Problem Definition}

There are three main issues in traditional reinforcement learning curve. Because it has smooth power law with diminishing gain derived from average value, all possible individual learning curve are squashed into one learning curve. Thus, it cannot tell which learning curve will belong to a task, behaviour, and learning process. Furthermore, even though identifying bad or good on learnings are important, but it cannot provide sufficient information about those.

Below is the list of problems on current reinforcement learning curve.

\begin{itemize}
\item Not being able to categorise learning process.
\item Averaging possible individual learning curves
\item Not enough information for identification of learning success
\end{itemize}
%Explaining about the problem of bootstrapping analysis.\\
%https://en.wikipedia.org/wiki/Bootstrapping (statistics)\\
% "Unless one is reasonably sure that the underlying distribution is not heavy tailed, one should hesitate to use the naive bootstrap"\\
%Bootstrap of the mean in the infinite variance case Athreya, K.B. Ann Stats vol 15 (2) 1987 724-731\\

%%%%%%%%%%%%%%%%%%%
% Piecewise powerlaw? ????? ??? ???? ??? curve? ?? ???? 
% ??? ? ????!
%%%%%%%%%%%%%%%%%%%

% This is a suggested section heading

% ======================================= %
%		1.3. Aims and Objectives
% ======================================= %

\section{Aims and Objectives}
%%%%%
These are the concepts I think you should cover in your introduction:

Skill acquisition \& expertise
- factors which are known to influence skill acquisition
- practice amount, practice spacing, others

Using games to study skill acquisition

Learning curves
- different possible underlying functions
- problems with averaging over

The ultimate aim, of course, is to identify individuals who learn faster or for longer, and try and relate this to how they practice
%%%%%
The aim of this paper is to fit different functions to the Axon game data. In the data, there are 854,064 individuals data about when, where they did the game and other informations, as well as scores of game according to their attempts. And so identify the underlying learning process. Furthermore, it is to inspect features which may have influences on the learning curves. 

\begin{itemize}
\item Understand several possible functions; step function, powerlaw, piecewise powerlaw and etc.
\item Understand different underlying learning processes; insight learning, associative learning, multi-component/process learning and etc.
\item Fit functions to individual Axon game data and compare those to identify.
\item Test theories of what makes learning most effective, exploiting unsupervised learning, establish which parameter is important for getting learning curve from the data.
\item Design more effective learning practices.
\end{itemize}
% This section is compulsory -- guidelines can be found in the presentation about writing the dissertation

%The aim(s) should provide a general idea of what the project is about and the objectives should be more specific. The objectives should be actions that you expect to do (e.g. simulate..., test..., compare...). At the end of the project it should be possible to assess which of the objectives have been achieved. The objectives will also link into the work programme (below).

% ======================================= %
%		1.4. Project Management
% ======================================= %

\section{Project Management}

% This section is compulsory -- guidelines can be found in the presentation about writing the dissertation

Text...

% ======================================================== %
%		2. Data Curation
% ======================================================== %


\chapter{Data Curation}
A large number of data were acquired through the online game named Axon which is developed by Preload for Welcome trust \cite{stuart2009Axon}. Totally 1,201,516 machine identities (or players) played the game over 4 million times. The raw data set is fundamentally comprised by the score, date, time of plays and so on in accordance with machine identities which may represent each individual. %\cite{stafford2014tracing}. 
Undoubtedly, it can be possible to extract information from the data set on how people practiced to get a higher score. For example, it shows how much time they played for each score. 
However, the raw data set of on-line game seems quite noisy to discern those factors which affect player's learning. It contained many 'undefined' or 'unrepresentable values', and some values are not valid, such as starting play time is later than the time play finished. For this reason, the information on what the analysis needs cannot be identified directly from it.

Thus, the data source needs refining processes until the gemstone of data set could gleam with the evidence of valuable information. In other word, the big data set has to be curated. Stonebraker et al. stated that "data curation is the act of discovering a data source(s) of interest, cleaning and transforming the new data, semantically integrating it with other local data sources, and deduplicating the resulting composite" \cite{stonebraker2013data}.

\bigskip
Below are the tasks in which steps of data curation are largely expected,

% that this dissertation is interested in, and especially where
\begin{enumerate}
	\item Cleaning raw data set,
	%\item Grouping filtered data set according to the same machine ID,
	\item Incrementally accommodating new data entities,
	\item Normalising scores grouped by the same players,
	\item Clustering players, showing the similar patterns.
\end{enumerate}

Especially, data curation tasks followed methods of two previous studies
\cite{stafford2014tracing, stafford2016testing}.
Accordingly, the curation tasks of cleaning, grouping by machine IDs and accommodating will be explained at the following section. After then, another tasks for data deformation and clustering will be examined.

\begin{comment}
which certified "the relationship between the practice amount and subsequent performance, and between practice spacing and subsequent performance" and "the consolidation effect for skill learning". In order for these analyses, unnecessary information contained in the data set had to be filtered, and additionally, the new variables were augmented by manifesting some hidden features in the same player through those two analyses.

In gratitude, filtering methods on the data source was already regularized in some degree by Stafford et al. (2014; 2016).
\end{comment}
% , K-Means clustering, with the artificial neural network algorithm, competitive learning 

% ======================================= %
%		2.1. Data Cleaning
% ======================================= %
\section{Data Cleaning}

What has to be noted before data cleaning is that it has to be implemented under the grouped data by the same machine IDs. Totally 4,038,802 plays were sorted by players having the same machine IDs, and then 1,201,516 players were detected. The criteria according to the previous two studies \cite{stafford2014tracing, stafford2016testing} are as follow,
% Though the two studies in 2014 and 2016 about skill learning analysis using the game data source, some curation steps were performed by the following criteria.

%\begin{itemize}
%	\item 26,727 players who did not play a minimum of 15 games were eliminated,
%	\item 8\% out of the total number of players attempted more than 300 times, and those were also discharged.
%	\item 26,175 data which lack valid longitude or timing information for each attempt were filtered.
%\end{itemize}

\begin{enumerate}
	\item players who did not play at least 15 games,
	\item players who attempted more than 300 times,
	\item data which lack valid longitude or timing information for each attempt,
	\item players with discontinuous game play attempts.
\end{enumerate}

The reason of discarding those who played less than 15 attempts is because those cannot show a causal relationship between attempts and scores. In this step, xxx individuals were deleted. 
And the reason of second step is that plays more than 300 attempts did not show many differences against after 300 attempts. ??? out of total number of players attempted the game more than 300 times, and also they were removed. Also discarded players having unrecognisable time information were ???. Finally, game play data for each individual recorded discontinuously. In other word, there are non-recorded game play data between the first attempt and the last attempt for each players. The reason why this problem happened is unknown though, it is obvious that individuals with this issue have to be filtered appropriately. Though cleaning those is simply mentioned in this session, the specific method how to manage players with discontinuous data will be discussed in chapter ???.

As a result, raw data with 4, 038,802 plays with xxx players were cleaned, and then reduced to ??? plays and ??? individuals decreased by ???.

\section{Data accommodating}
The next step of data curation is to incrementally accommodate hidden but obvious features from the previous cleaned data set. It is originally constituted by basic 8 categories. After data accommodating task, 7 additional hidden variable would be derived.

\subsection{Original variables in the data source}
When people play the on-line game, 7 basic information as follow is recorded,
\begin{itemize}
    \item machine identities,
	\item scores and attempt numbers,
	\item date, hour and minute,
	\item latitude and longitude.
\end{itemize}

When players access the on-line game, tracking code written by Preloaded record machine identities. Machine IDs may be considered as individuals who actually play the game. Preloaded inserted tracking code to the on-line game, which records a machine identity at each time the game was loaded, as well as the result of the game at the end of plays, and play date and time. Attempts and scores were recorded in the order of the time.

The information of locations where the machine ID accessed the game is also collected approximately at the maximum of city-block level.

\subsection{Hidden variables calculated from the original variables}
From the basic variables which are recorded when people play the game, other features that may imply useful information of how players are influenced during learning processes to get higher marks can be derived as bellow,

\begin{itemize}
	\item time difference,
	\item local time,
	\item gap types (no, long, short and sleep gap),
	\item a number of total plays.
\end{itemize}

The basic time information; date, hour and minute is combined together. And then, time differences between successive attempts are calculated by subtracting current game start time from its previous attempt time. Therefore, the first attempts do not have value of time differences.

\bigskip
Next, local times for each play were calculated in seconds, using the formula, "$\textit{local time} = \textit{UTCtime} + (\textit{longitude} × 24\div360),\textit{modulo}24$" \cite{stafford2016testing}. Local time would facilitate comparison of players in different time zone at the same level as life style time. There was data that local time could not be calculated because of missing tack of latitude and longitude, and those were filtered at the data cleaning task.

\bigskip
On the basis of time difference, Stafford et al. (2016) classified individuals into 4 groups, according to "the nature of the timing of their first 15 attempts at the game". First group is 'no gap' for those who had less than 15 minute break between each play. Second group is 'wake' in which players are assumed having a rest in there waking hours if they have "a single gap between 7 and 12 hours". Working hours is time between 5 am and 12 pm. And those who rested for between 7 and 12 hours in working hours were categorised as 'sleep'. Finally, all other individuals are classified as "no category".

Furthermore, for more detailed analysis, 'rest' which is defined in 'gap types' categorisation was divided with 3 types. Long gap, short gap and sleep gap were determined when players had a rest 'between 7 hours and 11 hours', 'below 15 minutes', and 'not in working hours' respectively.

\bigskip
As the term of hidden variables, there are possibilities that more factors can be mined from the data set and be incrementally accommodated to it if appropriate methods are implemented.

\section{Data Deformation}
CSV format has been used as a basic data file format for previous study.

CSV is an abbreviation of Comma-Separated Values. A data record of CSV format file is a line formed by one or more elements, and commas separate those element values to be distinguishable (plain text format) \cite{shafranovich2008common}. According to Idris (2014), it is straightforward for those format to load and store data-set from the data storage onto working place; ease to generate, read and edit manually, versatility in most programming languages, but not efficient because exploiting plain text format "take a lot of space". And this might result in consuming more time to handle such a big data set than using binary format.

Simply, the data set needs to be stored in memory unit of machine(s) where the analysis processes would operate. However, these are true only when data size is manageable. Because fitting all data-set of such a big data in computer memory takes many spaces. In some case loading on computer memory cannot be possible depending on computer capability and setup. Thus, CSV format is not very efficient way to manage such a big data \cite{idris2014python}. To solve with those problems, two approaches of being able to reduce storing spaces and of enabling faster speed for analysing were applied; converting CSV file format into Pickle format, and splitting data set into several pieces.

\subsection{Pickle file format}
Pickle file format can provide "a high level of data compression such as zip, bzip and gzip" and also faster than CSV \cite{idris2014python}, because it is "binary protocols for serializing and de-serializing a Python object structure" \cite{python2016pickle}. Furthermore, it provides easy approach to deal with converting Python object hierarchy into a binary stream and vice versa, with the notion of 'pickling' and 'unpickling'. Moreover, pickle file format can automatically represent a large number of Python types \cite{python2016pickle}.

\bigskip
Therefore, curated data set after cleaning, accommodating, and deformation process is stored as binary file format, and for the binary file format, pickle specified for Python is used.

\subsection{Data storage and index table}
Although the big data set can be converted to binary file, it still takes large amount of storage. And it also causes RAM memory optimisation problem. When data curation tasks performed (of course, as well as data analysis task which will be discussed in the next session), processing speed became slower and slower over time. This might be because data occupies all spaces of hardware memory, and exploits virtual memory on main memory unit. Basically, RAM memory provides very high speed compared to hard disk, and it is even faster than Solid State Drive (SSD) which is often considered 'fast'. If data takes all RAM memory spaces, it transfer addresses of some part of it to virtual memory. This task causes many interruptions to the tasks, and eventually decrease processing performance remarkably. And depending on operating systems, Python kernel may face crush.

\bigskip
Therefore, loading only necessary data to a specific task will be a solution to keep processing performance fast. Whole data set was separated by each type of variables. To secure not to lose player information within the divided data, address of each data has to be indexed directing player ID. As a result of data deformation task, 15 pickle data files were generated; eventLabel, date, hour, minute, latitude, longitude, eventValue, play\_filter, comb\_time, diff\_time, local\_time, long\_gap, shor\_gap, sleep\_gap, gap\_type, and total\_plays, which are pointed by individual ID information, eventAction.

%%%%%% comment %%%%%%%%%%%%%
\begin{comment}
(Python 2001)
There are various file formats which can provide a high level of data compression.

, which is formed from 15 factors, into the data vector having like hash table

By default, the pickle data format uses a relatively compact binary representation. If you need optimal size characteristics, you can efficiently compress pickled data.


?CSV is human readable and easy to edit manually
?CSV is simple to implement and parse
?CSV is processed by almost all existing applications
?CSV provides a straightforward information schema 
?CSV is faster to handle
?CSV is smaller in size
?CSV is considered to be standard format
?CSV is compact. For XML you start tag and end tag for each column in each row. In CSV you write the column headers only once.
?CSV is easy to generate

Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate data-set, store them on disk and curate them independently. Later we'll merge them into a single data-set of manageable size.
\end{comment}

% ======================================= %
%		2.4. K-Means Clustering with Competitive Learning Algorithm
% ======================================= %
\section{K-Means Clustering with Competitive Learning Algorithm}

In order to identify learning curves of individuals, the first thing to do is to cluster players into several groups in which they show similar learning patterns. Traditional ways of identifying individual learning curves is either averaging all data of test subjects or fitting different possible underlying functions to observed population data-set \cite{newell1981mechanisms, gallistel2004learning, donner2015piecewise}

\bigskip
Well represented learning curve in psychology might be a "smooth power law of diminishing gains" \cite{donner2015piecewise}. For example, Howard (2014) tested function fitting on chess performance and verified that "power function fit is best". 

However, in contrast, Gaschler et al. (2014) stated that "the exponential function" is "better than the power function fitting" for learning curves of chess players. This different results of the same observation of chess game would stem from that the power law is an artifact which averages many divergent shapes of exponential curves (Heathcote et al., 2000; Murre \& Chessa, 2011) as well as other possible underlying curves. 

Therefore, before analysing the on-line game data set, it would be better assumption that there are many different shapes of learning curves depicting individuals' learning best, rather than only one learning curve. 
% Hence, it would be better to cluster players by similar learning patterns before identifying which curve functions will fit best on the on-line game data-set.

\bigskip
In contrast with classification tasks that classify labelled data-set, clustering is an unsupervised classification task which automatically groups similar data without predefined and labelled classes \cite{hackeling2014mastering}, and it generates the same result as classification algorithm does \cite{harrington2012machine}. 

Among several clustering methods, K-Means clustering was used with artificial neural network algorithm, competitive learning.
%, for the analysis of the on-line game data set. 
It is because K-Means clustering algorithm in high dimensions is computationally very slow and the key method of K-Means; comparing distance in 2 and 3 dimensions, would not work for satisfying outcome in high dimensions \cite{daume2012course}.

\bigskip
Hence, there is possibility that the data-set may not properly be clustered, as the on-line game data-set is comprised of 15 dimensions. Compensating K-Means clustering with neural network algorithm would provide one of solutions for the high dimensions data-set.

% ======================================= %
%		2.4.1 K-Means clustering
% ======================================= %
\subsection{K-Means clustering}
The name of K-Means is because it discovers k unique clusters, and the mean values of that clusters represent the centres of each cluster. The key idea of the algorithm is to compare only distances between all data and arbitrary number of cluster centres. After centroids of clusters were found, each data can be assigned to its nearest centres. In the same manner, centres of clusters can be computed after each data is allocated to clusters.

Again, there are no labels or predefined any information of classes in the data set. %In other word, 
Thus, the information of which approach has to be implemented first between two approaches is unknown without doubt. The ways to tackle this are to start with initial random point, to iterate calculation of centre position, and to rearrange elements in clusters. Through iterating these three approaches until stopping criteria is satisfied, k means eventually converges to local optimisation, and comprise k number of clusters. 

\bigskip
There are two terminating criteria in K-Means clustering algorithm. The first one is using a threshold for the difference of cost functions (\ref{eq:k_means_cost_function}) through subsequent iterations (\ref{eq:threshold1}).

\begin{equation} \label{eq:threshold1}
	J_i - J_{i-1} < \theta \quad \textrm{, where} \; j \in K,
\end{equation}

and cost function $J$ is

\begin{equation} \label{eq:k_means_cost_function}
    J(\mathbf{\mu, C; D}) = \sum_n^N || x_n -\mu_{C_k} ||^2
    = \sum^K_{k=1} \sum_{i \in C_k} ||x_i - \mu_k||^2,
\end{equation}
where $\mu$ is mean of cluster, $\mathbf{D}$ is \ (the number of parameters) data-set, $\mathbf{C}$ is clusters in the data-set, $x$ is data, $N$ is the number of data, and $K$ is the number of clusters. 

The other one is to use a threshold for the difference of centre positions between successive iterations (\ref{eq:threshold2}).
\begin{equation} \label{eq:threshold2}
    x_i - x_{i-1} < \theta \quad \textrm{, where } i \; \textrm{in} \; C_k \,.
\end{equation}

\bigskip

Below is pseudo-code of K-Means clustering algorithm, 
\bigskip
\begin{algorithmic}
\FORALL{$k$ such that $ 0 \leq k < K$}
	 \STATE randomly initialise $\mu_k$
\ENDFOR
\smallskip
\WHILE{stopping criteria (\ref{eq:threshold1}) or (\ref{eq:threshold2})}
	\FORALL{$k \in K$}
		\FORALL{$i$ such that $ 0 \leq i < N$}
			\STATE $\mathbf{C}_k \leftarrow \underset{x}{\arg\min}|| \mu_k - x_i ||^2$ ,\\
		\ENDFOR
	\ENDFOR
	\smallskip
	\FORALL{$k \in K$}
		\STATE $\mu_k \leftarrow \textrm{mean}(\mathbf{C}_k)$
	\ENDFOR
\ENDWHILE
\RETURN $\mathbf{C, \mu}$
\end{algorithmic}
%After finding centers of clusters, each data are assigned to its nearest clusters.

\bigskip
At first, the positions of centroids are randomly initialised. Then, the first for loop in the while loop assigns each data to nearest clusters. The second for loop calculates the centres of clusters. By iterating these two tasks, all centroid of clusters move slightly towards the centres of each cluster, and eventually converges to local optima. For on-line game data analysis, stopping criterion (\ref{eq:threshold2}) was used. 

Moreover, this K-Means clustering algorithm was improved with artificial neural network, competitive learning algorithm, in order to cluster the on-line game data-set in high dimensions.

% ======================================= %
%		2.4.2 Competitive learning
% ======================================= %
\subsection{Competitive learning}

Competitive learning is unsupervised learning approaches in which the output neurons contend against each other to react as the one firing from the given pattern in inputs. Therefore only one unit take winner upon all output units (\ref{fig:Competitive_Diagram}). This exclusive phenomenon in synaptic neurons is called 'winner-take-all units', and sometimes they also called 'grandmother cells'. Which means a neuron fires on a specific stimulus. This neural networks has an advantage of clustering unlabelled data and searching the correlations of the input data. One of the most significant application of competitive learning might be vector quantisation. In which, input data-set becomes a bunch of vectors in data space, and input vectors are compressed and stored by transmitting into prototype vectors.

\begin{figure}[ht]
    \centering
    \framebox{\parbox[t][0.7\textwidth][t]{0.95\textwidth}{
        \begin{center}
            \includegraphics[width=0.85\textwidth, height=0.65\textwidth]{Competitive_Diagram.png}
        \end{center} }}
	\caption{Competitive neuron diagram. When an input enters the network, it generates output values according to weights between an input and output neuron (blue arrows). Output neurons compete each others (red arrows), then only one output fires on an input pattern. Output neurons are considered only having binary $0/1$ output.}
	\label{fig:Competitive_Diagram}
\end{figure}

\bigskip
It has very similar concept to K-Means clustering algorithm. The prototype vectors can be interpreted as centroids in k-means. Besides, vector quantisation employs Euclidean metric to assign an input vector to the nearest prototype, weight vector. When an input data, usually represented as a vector, is applied at the input of the network, the winner neuron indicates the appropriate class. 

If all input data are entered to the network, the input space organises a Voronoi tessellation. To be specific, Voronoi tessellation which was used for the analysis is Centroid Voronoi tessellation, in which means are positioned in each Voronoi cell (centre of mass). The weight vectors (or centroid in K-Means) are the vectors of prototypes. 

\bigskip
Output neurons $O_i$ in (\ref{eq:competitive1}) are the units having the net input in an pattern at a time,
\begin{equation}\label{eq:competitive1}
    O_i = \sum_j w_{ij}x_j = \vec{\mathbf{w_i}} \cdot \vec{\mathbf{x}},
\end{equation}
for an input vector $\mathbf{x}$. 

Winner neurons are found by (\ref{eq:competitive2}),
\begin{equation} \label{eq:competitive2}
    \vec{\mathbf{w}}_k^T\vec{\mathbf{x}}^\mu > \vec{\mathbf{w}}_i^T\vec{\mathbf{x}}^\mu \quad \textrm{for all i},
\end{equation}
where winner neuron is $\vec{\mathbf{w}}_k^T$, and $\vec{\mathbf{x}}^\mu$ is an input of pattern $\mu$.

\bigskip
(\ref{eq:competitive2}) tells winning unit is the biggest. If the weight vectors connecting between each input and output are normalized, $\vec{\mathbf{w_i}}=1$, then (\ref{eq:competitive1}) can be redefined as below,
\begin{equation} \label{eq:competitive3}
    \quad |\vec{x}^\mu - \vec{w}_k^T| \leq |\vec{x}^\mu - \vec{w}_i^T|, \quad
    \textrm{only if} \;\; |\vec{w}=1| \;\; \textrm{for all i.}
\end{equation}

\bigskip
(\ref{eq:competitive3}) shows similarity to the key idea of K-Means clustering, which only consider distances between every data and centroids.

\bigskip
Competitive learning only consider the output neuron with the maximum net value of inner product between input and weight. Biologically, output neurons are connected with other all output neurons with lateral inhibition, as well as excitatory connection itself, and therefore winner takes all units.
%output neurons have a value of $\vec{w}^T\vec{x}^\mu$, thus the largest output is always win. 

\bigskip
For competitive learning, the thing to note is that weights must be normalised in order for all units to have a unit length.

% ======================================= %
%		2.4.3 Normalisation
% ======================================= %

\subsection{Normalisation}
Normalisation is an important for analysing data with different scale to be compared and then to derive information in K-Means clustering \cite{bowles2015machine}. To be specific about normalisation for competitive learning, the network exploits the inner product for each input with weight vectors in order to identify winner output unit. If looking inside of (\ref{eq:competitive2}), the equation for inner product is
\begin{equation}\label{eq:inner_product}
    \vec{\mathbf{w}}^T \cdot \vec{\mathbf{x}}^\mu = \vec{\mathbf{w}}^T\vec{\mathbf{x}}^\mu \cos(\theta),
\end{equation}
with respect to the lengths of both units and a direction between input pattern and weight. Not-normalised one has no way for an criterion for competing output units with weight vectors. Because the neuron which has the most largest value in length always wins, and that implies the algorithm will finish without any learning or no informative outcome through iterations in the both case of not-normalised data-set and weight vectors.

\begin{figure}[t]
\begin{subfigure}{0.5\textwidth}
    \centering
    \framebox{\parbox{0.95\textwidth}{
        \begin{center}
            \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{not_norm_data.png}
        \end{center} }}
    \caption{Not-normalised data with a unit weight (red arrow)}
    \label{fig:not_norm_data}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
    \framebox{\parbox{0.95\textwidth}{
        \begin{center}
            \includegraphics[width=0.9\textwidth, height=0.9\textwidth]{not_norm_weight.png}
        \end{center} }}
    \caption{Normalised data-set with not-normalised weight vectors (green arrow).}
    \label{fig:not_norm_weight}
\end{subfigure}
\caption{Examples of not-normalised data and weight in 3 dimensions. In both left and right figures, sphere represents unit length area.}
\label{fig:not-normalised}
\end{figure}

\bigskip
For the solution of that, normalising input and weight vectors simply transforms (\ref{eq:inner_product}) into the equation computing only a direction between those vectors, which tells how close weight vector is from the input vector. 

Eventually, the inner product in (\ref{eq:competitive3}) becomes the criterion which detects the shortest Euclidean distance by comparing angles between input and weight vectors, (\ref{eq:distance}).

\begin{equation}\label{eq:distance}
    \begin{split}
        \vec{\mathbf{w}}_k^T \cdot \vec{\mathbf{x}}^\mu &> 
        \vec{\mathbf{w}}_i^T \cdot \vec{\mathbf{x}}^\mu \\
        \cos(\theta_k) &\geq \cos(\theta_i).
    \end{split}
\end{equation}

The firing output neuron is still has the largest value among output units when the $\theta$ is smaller than others based on the competition with proximity of the input vector.

(\ref{fig:not-normalised}) shows not-normalised issue in competitive learning. In the not-normalised data-set and weight vector, (\ref{eq:inner_product}) always compares only the length, not the closest one in the distance between data and weight vector. It is obvious that the most long vector always win, which cannot be representative for input patters. Thus weights would not move to the centre of Voronoi cell.

%\nocite{*}
\bibliographystyle{apacite}
%\bibliographystyle{apalike}
\bibliography{references}
\end{document}